<html>
	<head>
		<meta charset="UTF-8">
		<link rel="stylesheet" href="css/main.css" type="text/css">
		<link rel="stylesheet" href="css/monokai_sublime.css" type="text/css">
		<script type="text/javascript" src="js/highlight.pack.js"></script> 
		<script>hljs.initHighlightingOnLoad();</script>
	</head>
	<body>
		<p>Stuff goes bad. Erlang in anger</p>
		<p>Contents</p>
		<a href="#introduction"><p>Introduction</p></a>
		<p><a href="#i_writing_application">I Writing Applications</a></p>
		<ul>
			<li><a href="#chapter_1">Chapter 1 "How to Dive into a Code Base"</a>
				<ul>
					<li><a href="#1_1_raw_erlang"> 1.1 Raw Erlang </a></li>
					<li>
						<a href="#1_2_otp_applications"> 1.2 OTP Applications</a>
						<ul>
							<li><a href="#1_2_1_library_applications"> 1.2.1 Library Applications</a></li>
							<li><a href="#1_2_2_regular_applications">1.2.2. Regular Applications</a></li>
							<li><a href="#1_2_3_dependencies">1.2.3. Dependencies</a></li>
						</ul>
					</li>
					<li><a href="#1_3_otp_releases">1.3 OTP Releases</a></li>
					<li><a href="#1_4_exercises">1.4 Exercises</a></li>
				</ul>
			</li>
			<li>
				<a href="#chapter_2">Chapter 2 "Building Open Source Erlang Software"</a>
				<ul>
					<li><a href="#2_1_project_structure">2.1 Project Structure</a> </li>
					<ul>
						<li><a href="#2_1_1_otp_applications">2.1.1 OTP Applications</a></li>
						<li><a href="#2_1_2_otp_releases4">2.1.2 OTP Releases</a></li>
					</ul>
					<li><a href="#2_2_supervisors_and_start_link_semantics">2.2 Supervisors and start_link Semantics</a></li>
					<ul>
						<li><a href="#2_2_1_its_about_the_guarantees">2.2.1 It’s About the Guarantees</a></li>
						<li><a href="#2_2_2_side_effects">2.2.2 Side Effects</a></li>
						<li><a href="#2_2_3_example_initializing">2.2.3 Example: Initializing without guaranteeing connections</a></li>
						<li><a href="#2_2_4_in_a_nutshell">2.2.4 In a nutshell</a></li>
						<li><a href="#2_2_5_application_strategies">2.2.5 Application Strategies</a></li>
					</ul>
					<li><a href="#2_3_exercises">2.3 Exercises</a></li>
				</ul>
			</li>
			<li>
				<a href="#chapter_3">Chapter 3  "Planning for Overload"</a>
				<ul>
					<li>
						<a href="#3_1_common_overload_sources">3.1 Common Overload Sources</a>
						<ul>
							<li><a href="#3_1_1_error_logger_explodes">3.1.1 error_logger Explodes</a></li>
							<li><a href="#3_1_2_locks_and_blocking_operations">3.1.2 Locks and Blocking Operations</a></li>
							<li><a href="#3_1_3_unexpected_messages">3.1.3 Unexpected Messages</a></li>
						</ul>
					</li>
				</li>
				<li>
					<a href="#3_2_restricting_input">3.2 Restricting Input</a>
					<ul>
						<li><a href="#3_2_1_how_long_should_timeout_be">3.2.1 How Long Should a Time Out Be</a></li>
						<li><a href="#3_2_2_asking_for_permission">3.2.2 Asking For Permission</a></li>
						<li><a href="#3_2_3_what_users_see">3.2.3 What Users See</a></li>
					</ul>
				</li>
				<li>
					<a href="#3_3_discarding_data">3.3 Discarding Data</a>
					<ul>
						<li><a href="#3_3_1_random_drop">3.3.1 Random Drop</a></li>
						<li><a href="#3_3_2_queue_buffers">3.3.2 Queue Buffers</a></li>
						<li><a href="#3_3_3_stack_buffers">3.3.3 Stack Buffers</a></li>
						<li><a href="#3_3_4_time_sensitive_buffers">3.3.4 Time-Sensitive Buffers</a></li>
						<li><a href="#3_3_5_dealing_with_constant_overload">3.3.5 Dealing With Constant Overload</a></li>
						<li><a href="#3_3_6_how_to_drop">3.3.6 How Do You Drop</a></li>
					</ul>
				</li>
				<li><a href="#3_4_exercises">3.4 Exercises</a></li>
			</ul>
		</ul>
		<p><a href="#ii_diagnosing_applications">II Diagnosing Applications</p>
		<ul>
			<li>
				<a href="#chapter_4">Chapter 4  "Connecting to Remote Nodes"</a>
				<ul>
					<li><a href="#4_1_job_control_mode">4.1 Job Control Mode</a></li>
					<li><a href="#4_2_remsh">4.2 Remsh</a></li>
					<li><a href="#4_3_ssh_daemon">4.3 SSH Daemon</a></li>
					<li><a href="#4_4_named_pipes">4.4 Named Pipes</a></li>
					<li><a href="#4_5_exercises">4.5 Exercises</a></li>
				</ul>
			</li>
			<li>
				<a href="#chapter_5">Chapter 5  "Runtime Metrics"</a>
				<ul>
					<li><a href="#5_1_global_view">5.1 Global View</a></li>
					<ul>
						<li><a href="#5_1_1_memory">5.1.1 Memory</a></li>
						<li><a href="#5_1_2_cpu">5.1.2 CPU</a></li>
						<li><a href="#5_1_3_processes">5.1.3 Processes</a></li>
						<li><a href="#5_1_4_ports">5.1.4 Ports</a></li>
					</ul>
					<li><a href="#5_2_digging_in">5.2 Digging In</a></li>
					<ul>
						<li><a href="#5_2_1_processes">5.2.1 Processes</a></li>
						<li><a href="#5_2_2_otp_processes">5.2.2 OTP Processes</a></li>
						<li><a href="#5_2_3_ports">5.2.3 Ports</a></li>
					</ul>
					<li><a href="#5_3_exercises">5.3 Exercises</a></li>
				</ul>
			</li>
			<li>
				<a href="#chapter_6">Chapter 6  "Reading Crash Dumps"</a>
				<ul>
					<li><a href="#6_1_general_view">6.1 General View</a></li>
					<li><a href="#6_2_full_mailboxes">6.2 Full Mailboxes</a></li>
					<li><a href="#6_3_too_many_processes">6.3 Too Many (or too few) Processes</a></li>
					<li><a href="#6_4_too_many_ports">6.4 Too Many Ports</a></li>
					<li><a href="#6_5_cant_allocate_memory">6.5 Can’t Allocate Memory</a></li>
					<li><a href="#6_6_exercises">6.6 Exercises</a></li>
				</ul>
			</li>
			<li>
				<a href="#chapter_7">Chapter 7  "Memory Leaks"</a>
				<ul>
					<li><a href="#7_1_common_sources_of_leaks">7.1 Common Sources of Leaks</a>
						<ul>
							<li><a href="#7_1_1_atom">7.1.1 Atom</a></li>
							<li><a href="#7_1_2_binary">7.1.2 Binary</a></li>
							<li><a href="#7_1_3_code">7.1.3 Code</a></li>
							<li><a href="#7_1_4_ets">7.1.4 ETS</a></li>
							<li><a href="#7_1_5_processes">7.1.5 Processes</a></li>
							<li><a href="#7_1_6_nothing_in_particular">7.1.6 Nothing in Particular</a></li>
						</ul>
						<li><a href="#7_2_binaries">7.2 Binaries</a></li>
						<ul>
							<li><a href="#7_2_1_detecting_leaks">7.2.1 Detecting Leaks</a></li>
							<li><a href="#7_2_2_fixing_leaks">7.2.2 Fixing Leaks</a></li>
						</ul>
						<li><a href="#7_3_memory_fragmentation">7.3 Memory Fragmentation</a></li>
						<ul>
							<li><a href="#7_3_1_finding_fragmentation">7.3.1 Finding Fragmentation</a></li>
							<li><a href="#7_3_2_erlang_memory_model">7.3.2 Erlang’s Memory Model</a></li>
							<li><a href="#7_3_3_fixing_memory_fragmentation">7.3.3 Fixing Memory Fragmentation with a Different Allocation Strategy</a></li>
						</ul>
						<li><a href="#7_4_exercises">7.4 Exercises</a></li>
					</ul>
				</li>
				
				<li>
					<a href="#chapter_8">Chapter 8  "CPU and Scheduler Hogs"</a>
					<ul>
						<li><a href="#8_1_profiling_and_reduction_counts">8.1 Profiling and Reduction Counts</a></li>
						<li><a href="#8_2_system_monitors">8.2 System Monitors</a></li>
						<ul>
							<li><a href="#8_2_1_suspended_ports">8.2.1 Suspended Ports</a></li>
						</ul>
						<li><a href="#8_3_exercises">8.3 Exercises</a></li>
					</ul>
				</li>
				<li>
					<a href="#chapter_9">Chapter 9  "Tracing"</a>
					<ul>
						<li><a href="#9_1_tracing_principles">9.1 Tracing Principles</a></li>
						<li><a href="#9_2_example_sessions">9.2 Tracing with Recon</a></li>
						<li><a href="#9_3_example_sessions">9.3 Example Sessions</a></li>
						<li><a href="#9_4_exercises">9.4 Exercises</a></li>
					</ul>
				</li>
				<li>
					<a href="#conclusion">Conclusion</a>
				</li>
			</ol>
			<hr>
			<a id="introduction"><h2>Introduction</h2></a>
			<a id="on_running_software"><h3>On Running Software</h3></a>
			<p>
			There’s something rather unique in Erlang in how it approaches failure compared to most other programming languages. There’s this common way of thinking where the language, programming environment, and methodology do everything possible to prevent errors. Something going wrong at run-time is something that needs to be prevented, and if it cannot be prevented, then it’s out of scope for whatever solution people have been thinking about.
			</p>
			<p>
			The program is written once, and after that, it’s off to production, whatever may happen there. If there are errors, new versions will need to be shipped.
			</p>
			<p>Erlang, on the other hand, takes the approach that failures will happen no matter what, whether they’re developer-, operator-, or hardware-related. It is rarely practical or even possible to get rid of all errors in a program or a system.<sup><a href="#1_sup1">1</a></sup> If you can deal with some errors rather than preventing them at all cost, then most undefined behaviours of a program can
			go in that "deal with it" approach.</p>
			<p>This is where the "Let it Crash" <sup><a href="#1_sup2">2</a></sup> idea comes from: Because you can now deal with failure, and because the cost of weeding out all of the complex bugs from a system before it hits production is often prohibitive, programmers should only deal with the errors they know how to handle, and leave the rest for another process (a supervisor) or the virtual machine to deal with.
			</p>
			<p>
			Given that most bugs are transient <sup><a href="#1_sup3">3</a></sup>, simply restarting processes back to a state known to be stable when encountering an error can be a surprisingly good strategy.
			</p>
			<p>Erlang is a programming environment where the approach taken is equivalent to the human body’s immune system, whereas most other languages only care about hygiene to make sure no germ enters the body. Both forms appear extremely important to me. Almost every environment offers varying degrees of hygiene. Nearly no other environment offers the immune system where errors at run time can be dealt with and seen as survivable.
			</p>
			<p>
			Because the system doesn’t collapse the first time something bad touches it, Erlang/OTP also allows you to be a doctor. You can go in the system, pry it open right there in production, carefully observe everything inside as it runs, and even try to fix it interactively. To continue with the analogy, Erlang allows you to perform extensive tests to diagnose the problem and various degrees of surgery (even very invasive surgery), without the patients needing to sit down or interrupt their daily activities.
			</p>
			<p>
			This book intends to be a little guide about how to be the Erlang medic in a time of war. It is first and foremost a collection of tips and tricks to help understand where failures come from, and a dictionary of different code snippets and practices that helped developers debug production systems that were built in Erlang.
			</p>
			<sub>
			<p><a id="1_sup1">1</a>life-critical systems are usually excluded from this category</p>
			<p><a id="1_sup2">2</a> Erlang people now seem to favour "let it fail", given it makes people far less nervous.</p>
			<p><a id="1_sup3">3</a>  131 out of 132 bugs are transient bugs (they’re non-deterministic and go away when you look at them, and trying again may solve the problem entirely), according to Jim Gray in <a href="http://www.hpl.hp.com/techreports/tandem/TR-85.7.html">Why Do Computers Stop and What Can Be Done About It?</a></p>
			</sub>
			<a id="who_is_this_for"><h3>Who is this for?</h3></a>
			<p>This book is not for beginners. There is a gap left between most tutorials, books, training sessions, and actually being able to operate, diagnose, and debug running systems once they’ve made it to production. There’s a fumbling phase implicit to a programmer’s learning of a new language and environment where they just have to figure how to get out of the guidelines and step into the real world, with the community that goes with it.
			</p>
			<p>
			This book assumes that the reader is proficient in basic Erlang and the OTP framework. Erlang/OTP features are explained as I see fit — usually when I consider them tricky — and it is expected that a reader who feels confused by usual Erlang/OTP material will have an idea of where to look for explanations if necessary <sup><a href="#1_sup4">4</a></sup>.
			</p>
			<p>
			What is not necessarily assumed is that the reader knows how to debug Erlang software, dive into an existing code base, diagnose issues, or has an idea of the best practices about deploying Erlang in a production environment <sup><a href="#1_sup5">5</a></sup>
			</p>
			<a id="how_to_read_this_book"><h3>How To Read This Book</h3></a>
			<p>This book is divided in two parts.</p>
			<p><a href="#part_i">Part I</a> focuses on how to write applications. It includes how to dive into a code base
			<a href="#chapter_1">(Chapter 1)</a>, general tips on writing open source Erlang software <a href="#chapter_2">(Chapter 2)</a>, and how to	plan for overload in your system design <a href="chapter_3">(Chapter 3)</a>.</p>

			<p><a href="#part_ii">Part II</a> focuses on being an Erlang medic and concerns existing, living systems. It contains instructions on how to connect to a running node <a href="#chapter_4">(Chapter 4)</a>, and the basic runtime metrics available <a href="#chapter_5">(Chapter 5)</a>. It also explains how to perform a system autopsy using a crash dump <a href="#chapter_6">(Chapter 6)</a>, how to identify and fix memory leaks <a href="#chapter_7">(Chapter 7)</a>, and how to find runaway CPU usage <a href="#chapter_8">(Chapter 8)</a>. The final chapter contains instructions on how to trace Erlang function calls in production using recon <sup><a href="#1_sup6">6</a></sup> to understand issues before	they bring the system down <a href="#chapter_9">(Chapter 9)</a>.</p>
			<p>Each chapter is followed up by a few optional exercises in the form of questions or hands-on things to try if you feel like making sure you understood everything, or if you want to push things further.</p>
			<sub>
				<p><a id="1_sup4">4</a> I do recommend visiting <a href="http://learnyousomeerlang.com/">Learn You Some Erlang</a> or the regular <a href="http://www.erlang.org/doc.html">Erlang Documentation</a> if a free resource is required</p>
				<p><a id="1_sup5">5</a> Running Erlang in a screen or tmux session is not a deployment strategy.</p>
				<p>
				<a id="1_sup6">6</a> <a href="http://ferd.github.io/recon/">http://ferd.github.io/recon/</a> — a library used to make the text lighter, and with generally productionsafe functions.</p>
			</sub>
			<hr/>
			<a id="part_i"><h1> Part I </h1></a>
			<a id="i_writing_application"><h1> Writing Applications </h1></a>
			<a id="chapter_1"><h2> Chapter 1 </h2></a>
			<a id="how_to_dive_into_a_code_base"><h2> How to Dive into a Code Base </h2></a>
			
			<p>"Read the source" is one of the most annoying things to be told, but dealing with Erlang programmers, you’ll have to do it often. Either the documentation for a library will be incomplete, outdated, or just not there. In other cases, Erlang programmers are a bit similar to Lispers in that they will tend to write libraries that will solve their problems and not really test or try them in other circumstances, leaving it to you to extend or fix issues that arise in new contexts.</p>
			
			<p>It’s thus pretty much guaranteed you’ll have to go dive in some code base you know nothing about, either because you inherited it at work, or because you need to fix it or understand it to be able to move forward with your own system. This is in fact true of most languages whenever the project you work on is not one you designed yourself.</p>
			<p>There are three main types of Erlang code bases you’ll encounter in the wild: raw Erlang code bases, OTP applications, and OTP releases. In this chapter, we’ll look at each of these and try to provide helpful tips on navigating them.</p>
			
			<a id="raw_erlang"><h3>1.1. Raw Erlang</h3></a>
			
			<p>If you encounter a raw Erlang code base, you’re pretty much on your own. These rarely follow any specific standard, and you have to dive in the old way to figure out whatever happens in there.</p>
			
			<p>This means hoping for a <span class="code">README.md</span> file or something similar that can point to an entry point in the application, and going from there, or hoping for some contact information that can be used to ask questions to the author(s) of the library.</p>
			
			<p>Fortunately, you should rarely encounter raw Erlang in the wild, and they are often beginner projects, or awesome projects that were once built by Erlang beginners and now need a serious rewrite. In general, the advent of tools such as <a href="https://github.com/rebar/rebar/"><span class="code">rebar</span></a><sup><a href="#sub_1_1">1</a></sup> made it so most people use OTP Applications.</p>
			<sub>
			  <a id="sub_1_1">1</a> <a href="https://github.com/rebar/rebar/">https://github.com/rebar/rebar/</a> — a build tool briefly introduced in <a href="#chapter_2">Chapter 2</a>
			</sub>
			<a id="1_2_otp_applications"><h3>1.2 OTP Applications</h3></a>
			<p>Figuring out OTP applications is usually rather simple. They usually all share a directory structure that looks like:</p>
<pre>
<code class="erlang">doc/
ebin/
src/
test/
LICENSE.txt
README.md
rebar.config</code>
</pre>
			<p>There might be slight differences, but the general structure will be the same.</p>
			<p>Each OTP application should contain an <span class="it">app file</span>, either <span class="code">ebin/&ltAppName&gt.app</span> or more often, <span class="code">src/&ltAppName&gt.app.src</span> <sup><a href="#sub_1_2">2</a></sup>. There are two main varieties of <span class="code">app</span> files:</p>
<pre>
<code class="erlang">{application, useragent, [
{description, "Identify browsers & OSes from useragent strings"},
{vsn, "0.1.2"},
{registered, []},
{applications, [kernel, stdlib]},
{modules, [useragent]}
]}.
</code>
</pre>
					<p>And.</p>
<pre>
<code class="erlang" >{application, dispcount, [
{description, "A dispatching library for resources and task "
"limiting based on shared counters"},
{vsn, "1.0.0"},
{applications, [kernel, stdlib]},
{registered, []},
{mod, {dispcount, []}},
{modules, [dispcount, dispcount_serv, dispcount_sup,
dispcount_supersup, dispcount_watcher, watchers_sup]}
]}.
</code>
</pre>
					<p>This first case is called a <span class="it">library application</span>, while the second case is a regular <span class="it">application</span>.</p>
					<sub>
						<a id="sub_1_2">2</a> A build system generates the final file that goes in <span class="code">ebin</span>. Note that in these cases, many <span class="code">src/&ltAppName&gt.app.src</span> files do not specify modules and let the build system take care of it.
					</sub>
					<a id="1_2_1_library_applications"><h4>1.2.1 Library Applications</h4></a>
						<p>Library applications will usually have modules named <span class="it">appname</span> <span class="code">_something</span>, and one module named <span class="it">appname</span> . This will usually be the interface module that’s central to the library and contains a quick way into most of the functionality provided.</p>

						<p>	By looking at the source of the module, you can figure out how it works with little effort: If the module adheres to any given behaviour (<span class="code">gen_server</span>, <span class="code">gen_fsm</span>, etc.), you’re most likely expected to start a process under one of your own supervisors and call it that way. If no behaviour is included, then you probably have a functional, stateless library on your hands. For this case, the module’s exported functions should give you a quick way to understand its purpose.</p>
					<a id="1_2_2_regular_applications"><h4>1.2.2 Regular Applications</h4></a>
						<p>For a regular OTP application, there are two potential modules that act as the entry point:</p>

						<ol>
							<li><span class="it">appname</span></li>
							<li><span class="it">appname</span> <span class="code">_app</span></li>
						</ol>

						<p>The first file should be similar in use to what we had in a library application (an entry point), while the second one will implement the application behaviour, and will represent the top of the application’s process hierarchy. In some cases the first file will play both roles at once.</p>

						<p>If you plan on simply adding the application as a dependency to your own app, then look inside <span class="it">appname</span> for details and information. If you need to maintain and/or fix the application, go for <span class="it">appname</span> <span class="code">_app</span> instead.</p>
						
						<p>The application will start a top-level supervisor and return its <span class="code">pid</span>. This top-level supervisor will then contain the specifications of all the child processes it will start on its own <sup><a href="#sub_1_3">3</a></sup>.</p>
						
						<p>The higher a process resides in the tree, the more likely it is to be vital to the survival of the application. You can also estimate how important a process is by the order it is started (all children in the supervision tree are started in order, depth-first). If a process is started later in the supervision tree, it probably depends on processes that were started earlier.</p>

						<p>Moreover, worker processes that depend on each other within the same application (say, a process that buffers socket communications and relays them to a finite-state machine in charge of understanding the protocol) are likely to be regrouped under the same supervisor and to fail together when something goes wrong. This is a deliberate choice, as it is usually simpler to start from a blank slate, restarting both processes, rather than trying to figure out how to recuperate when one or the other loses or corrupts its state.</p>

						<sub>
							<a id="sub_1_3">3</a> In some cases, the supervisor specifies no children: they will either be started dynamically by some function of the API or in a start phase of the application, or the supervisor is only there to allow OTP environment variables (in the <span class="code">env</span> tuple of the app file) to be loaded.
						</sub>

						<p>The supervisor restart strategy reflects the relationship between processes under a supervisor:</p>

						<ul>
							<li>
								<span class="code">one_for_one</span> and <span class="code">simple_one_for_one </span> are used for processes that are not dependent upon each other directly, although their failures will collectively be counted towards total application shutdown <sup><a href="#sub_1_4">4</a></sup>.
							</li>
							<li><span class="code">rest_for_one</span> will be used to represent processes that depend on each other in a linear manner.</li>
							<li><span class="code">one_for_all</span> is used for processes that entirely depend on each other.</li>
						</ul>
						
						<p>This structure means it is easiest to navigate OTP applications in a top-down manner by exploring supervision subtrees.</p>
						<p>For each worker process supervised, the behaviour it implements will give a good clue about its purpose:</p>
						
						<ul>
							<li>a <span class="code">gen_server</span> holds resources and tends to follow client/server patterns (or more generally, request/response patterns)</li>
							<li>a <span class="code">gen_fsm</span> will deal with a sequence of events or inputs and react depending on them, as a Finite State Machine. It will often be used to implement protocols.</li>
							<li>a <span class="code">gen_event</span> will act as an event hub for callbacks, or as a way to deal with notifications of some sort.</li>
						</ul>
						
						<p>All of these modules will contain the same kind of structure: exported functions that
						represent the user-facing interface, exported functions for the callback module, and private functions, usually in that order.</p>
						
						<p>Based on their supervision relationship and the typical role of each behaviour, looking at the interface to be used by other modules and the behaviours implemented should reveal a lot of information about the program you’re diving into.</p>
						
						<a id="1_2_3_dependencies"><h4>1.2.3 Dependencies</h4></a>
						<p>All applications have dependencies <sup><a href="#sub_1_5">5</a></sup>, and these dependencies will have their own dependencies. OTP applications usually share no state between them, so it’s possible to know what bits of code depend on what other bits of code by looking at the app file only, assuming the developer wrote them in a mostly correct manner. <a href="#figure_1_1"> Figure 1.1 </a> shows a diagram that can be generated from looking at app files to help understand the structure of OTP applications.</p>
						
						<p>Using such a hierarchy and looking at each application’s short description might be helpful to draw a rough, general map of where everything is located. To generate a similar diagram, find <span class="code">recon</span>’s script directory and call <span class="code">escript script/app_deps.erl</span> <sup><a href="#sub_1_6">6</a></sup>. Similar hierarchies can be found using the observer <sup><a href="#sub_1_7">7</a></sup> application, but for individual supervision trees. Put together, you may get an easy way to find out what does what in the code base.</p>

						<sub>
							<p><a id="sub_1_4">4</a> Some developers will use <span class="code">one_for_one</span> supervisors when <span class="code">rest_for_one</span> is more appropriate. They require strict ordering to boot correctly, but forget about said order when restarting or if a predecessor dies.</p>
							<p><a id="sub_1_5">5</a> At the very least on the <span class="code">kernel</span> and <span class="code">stdlib</span> applications</p>
							<p><a id="sub_1_6">6</a> This script depends on graphviz</p>
							<p><a id="sub_1_7">7</a> <a href="http://www.erlang.org/doc/apps/observer/observer_ug.html">http://www.erlang.org/doc/apps/observer/observer_ug.html</a></p>
						</sub>
						<a id="figure_1_1"><img src="images/riak_arc.png"></a>
						<p>Figure 1.1: Dependency graph of riak_cs, Basho’s open source cloud library. The graph ignores dependencies on common applications like kernel and stdlib. Ovals are applications, rectangles are library applications.</p>
						
						<a id="1_3_otp_releases"><h3>1.3 OTP Releases</h3></a>

						<p>OTP releases are not a lot harder to understand than most OTP applications you’ll encounter in the wild. A release is a set of OTP applications packaged in a production-ready manner so it boots and shuts down without needing to manually call <span class="code">application:start/2</span> for any app. Of course there’s a bit more to releases than that, but generally, the same discovery process used for individual OTP applications will be applicable here.</p>
						
						<p>You’ll usually have a file similar to the configuration files used by <span class="code">systools</span> or <span class="code">reltool</span>, which will state all applications part of the release and a few <sup><a href="#sub_1_8">8</a></sup> options regarding their packaging. To understand them, I recommend reading existing documentation on them. If you’re lucky, the project may be using <span class="code">relx</span><sup><a href="#sub_1_9">9</a></sup>, an easier tool that was officially released in early 2014.</p>
						<sub>
							<p><a id="sub_1_8">8</a> A lot</p>
							<p><a id="sub_1_9">9</a> <a href="https://github.com/erlware/relx/wiki">https://github.com/erlware/relx/wiki</a></p>
						</sub>
						<a id="1_4_exercises"><h3>1.4 Exercises</h3></a>
						<a id="1_4_review_questions"><h4>Review Questions</h4></a>
						<ol>
							<li>How do you know if a code base is an application? A release?</li>
							<li>What differentiates an application from a library application?</li>
							<li>What can be said of processes under a <span class="code">one_for_all</span> scheme for supervision?</li>
							<li>Why would someone use a <span class="code">gen_fsm</span> behaviour over a <span class="code">gen_server</span>?</li>
						</ol>
						<a id="1_4_hands_on"><h4>Hands-on</h4></a>
						<p>Download the code at <a href="https://github.com/ferd/recon_demo">https://github.com/ferd/recon_demo</a>. This will be used as a test bed for exercises throughout the book. Given you are not familiar with the code base yet, let’s see if you can use the tips and tricks mentioned in this chapter to get an  understanding of it.</p>
						<ol>
							<li>Is this application meant to be used as a library? A standalone system?</li>
							<li>What does it do?</li>
							<li>Does it have any dependencies? What are they?</li>
							<li>The app’s <span class="code">README</span> mentions being non-deterministic. Can you prove if this is true? How?</li>
							<li>Can you express the dependency chain of applications in there? Generate a diagram of them?</li>
							<li>Can you add more processes to the main application than those described in the <span class="code">README</span>?</li>
						</ol>
						
						<a id="chapter_2"><h2>Chapter 2</h2></a>
						<h2>Building Open Source Erlang	Software</h2>
						
						<p>Most Erlang books tend to explain how to build Erlang/OTP applications, but few of them go very much in depth about how to integrate with the Erlang community doing Open Source work. Some of them even avoid the topic on purpose. This chapter dedicates itself to doing a quick tour of the state of affairs in Erlang.</p>
						
						<p>OTP applications are the vast majority of the open source code people will encounter. In fact, many people who would need to build an OTP release would do so as one umbrella OTP application.</p>
						
						<p>If what you’re writing is a stand-alone piece of code that could be used by someone building a product, it’s likely an OTP application. If what you’re building is a product that stands on its own and should be deployed by users as-is (or with a little  configuration), what you should be building is an OTP release. <sup><a href="#sub_2_1">1</a></sup></p>
						
						<p>The main build tools supported are <span class="code">rebar</span> and <span class="code">erlang.mk</span>. The former is a portable Erlang script that will be used to wrap around a lot of standard functionality and add its own, while the latter is a very fancy makefile that does a bit less, but tends to be faster when it comes to compiling. In this chapter, I’ll mostly focus on using <span class="code">rebar</span> to build things, given it’s the ad-hoc standard, is well-established, and <span class="code">erlang.mk</span> applications tend to also be supported by <span class="code">rebar</span> as dependencies.</p>
						<sub>
							<p><a id="sub_2_1">1</a> The details of how to build an OTP application or release is left up to the Erlang introduction book you have at hand.</p>
						</sub>
						<a id="2_1_project_structure"><h3>2.1 Project Structure</h3></a>
						<p>The structures of OTP applications and of OTP releases are different. An OTP application can be expected to have one top-level supervisor (if any) and possibly a bunch of dependencies that sit below it. An OTP release will usually be composed of multiple OTP applications, which may or may not depend on each other. This will lead to two major ways to lay out applications.</p>
						<a id="2_1_1_otp_applications"><h4>2.1.1 OTP Applications</h4></a>
						<p>For OTP applications, the proper structure is pretty much the same as what was explained in 1.2:</p>
<pre><code class="erlang">1 doc/
2 deps/
3 ebin/
4 src/
5 test/
6 LICENSE.txt
7 README.md
8 rebar.config</code></pre>
						<p>What’s new in this one is the <span class="code">deps/</span> directory, which is fairly useful to have, but that
						will be generated automatically by <span class="code">rebar</span> <sup><a href="#sub_2_2">2</a></sup>  if necessary. That’s because there is no canonical package management in Erlang. People instead adopted <span class="code">rebar</span>, which fetches dependencies locally, on a per-project basis. This is fine and removes a truckload of conflicts, but means that each project you have may have to download its own set of dependencies</p>
						<p>This is accomplished with <span class="code">rebar</span> by adding a few config lines to <span class="code">rebar.config</span>:</p>
						
<pre><code class="erlang">1 {deps,
2 [{application_name, "1.0.*",
3  {git, "git://github.com/user/myapp.git", {branch,"master"}}},
4 {application_name, "2.0.1",
5  {git, "git://github.com/user/hisapp.git", {tag,"2.0.1"}}},
6 {application_name, "",
7  {git, "https://bitbucket.org/user/herapp.git", "7cd0aef4cd65"}},
8 {application_name, "my regex",
9  {hg, "https://bitbucket.org/user/theirapp.hg" {branch, "stable"}}}]}.</code></pre>
						<sub>
							<a id="sub_2_2">2</a> A lot of people package <span class="code">rebar</span> directly in their application. This was initially done to help people who had never used rebar before use libraries and projects in a boostrapped manner. Feel free to install rebar globally on your system, or keep a local copy if you require a specific version to build your system.
						</sub>
						<p>Applications are fetched directly from a <span class="code">git</span> (or <span class="code">hg</span>, or <span class="code">svn</span>) source, recursively. They can then be compiled, and specific compile options can be added with the <span class="code">{erl_opts, List}</span>.
						option in the config file <sup><a href="#sub_2_3">3</a></sup>.</p>
						<p>Within these directories, you can do your regular development of an OTP application. To compile them, call <span class="code">rebar get-deps compile</span>, which will download all dependencies, and then build them and your app at once.</p>
						<p>When making your application public to the world, distribute it <span class="it">without</span> the dependencies. It’s quite possible that other developers’ applications depend on the same applications yours do, and it’s no use shipping them all multiple times. The build system in place (in this case, <span class="code">rebar</span>) should be able to figure out duplicated entries and fetch everything necessary only once.</p>
						<a id="2_1_2_otp_releases4"><h4>2.1.2 OTP Releases</h4></a>
						<p>For releases, the structure should be a bit different <sup><a href="#sub2_4">4</a></sup>. Releases are collections of applications, and their structures should reflect that</p>
						<p>Instead of having a top-level app, applications should be nested one level deeper and  divided into two categories: apps and deps. The apps directory contains your applications’ source code (say, internal business code), and the deps directory contains independently managed dependency applications.</p>
<pre><code class="erlang">apps/
doc/
deps/
LICENSE.txt
README.md
rebar.config
</code></pre>
						<p>This structure lends itself to generating releases. Tools such as Systool and Reltool have been covered before <sup><a href="#sub_2_5">5</a></sup>, and can allow the user plenty of power. An easier tool that recently appeared is <span class="code">relx</span> <sup><a href="#sub_2_6">6</a></sup>.</p>
						<p>A <span class="code">relx</span> configuration file for the directory structure above would look like:</p>
<pre>
<code class="erlang">1 {paths, ["apps", "deps"]}.
2 {include_erts, false}. % will use currently installed Erlang
3 {default_release, demo, "1.0.0"}.
4
5 {release, {demo, "1.0.0"},
6 [members,
7 feedstore,
8 ...
9 	recon]}.</code></pre>
						<sub>
							<p><a id="sub_2_3">3</a> More details by calling <span class="code">rebar help compile</span></p>
							<p><a id="sub_2_4">4</a> I say <span class="it">should</span> because many Erlang developers put their final system under a single top-level application (in <span class="code">src</span>) and a bunch of follower ones as dependencies (in <span class="code">deps</span>), which is less than ideal for distribution purposes and conflicts with assumptions on directory structures made by OTP. People who do that tend to build from source on the production servers and run custom commands to boot their applications.</p>
							<p><a id="sub_2_4">5</a> <a href="http://learnyousomeerlang.com/release-is-the-word">http://learnyousomeerlang.com/release-is-the-word</a></p>
							<p><a id="sub_2_6">6</a> <a href="https://github.com/erlware/relx/wiki">https://github.com/erlware/relx/wiki</a></p>
						</sub>
						<p>Calling <span class="code">./relx</span> (if the executable is in the current directory) will build a release, to be found in the <span class="code">_rel/</span> directory. If you really like using <span class="code">rebar</span>, you can build a release as part of the project’s compilation by using a rebar hook in <span class="code">rebar.config</span>:</p>
<pre><code>1 {post_hooks,[{compile, "./relx"}]}.</code></pre>
						<p>
						And every time <span class="code">rebar compile</span> will be called, the release will be generated.
						</p>
						<a id="2_2_supervisors_and_start_link_semantics"><h3>2.2 Supervisors and start_link Semantics</h3></a>
							<p>In complex production systems, most faults and errors are transient, and retrying an operation is a good way to do things — Jim Gray’s paper <sup><a href="#sub_2_7">7</a></sup> quotes <span class="it">Mean Times Between Failures</span> (MTBF) of systems handling transient bugs being better by a factor of 4 when doing this. Still, supervisors aren’t just about restarting.</p>
							<p>One very important part of Erlang supervisors and their supervision trees is that <span class="it">their start phases are synchronous</span>. Each OTP process has the potential to prevent its siblings and cousins from booting. If the process dies, it’s retried again, and again, until it works, or fails too often.</p>
							<p>That’s where people make a very common mistake. There isn’t a backoff or cooldown period before a supervisor restarts a crashed child. When a network-based application tries to set up a connection during its initialization phase and the remote service is down, the application fails to boot after too many fruitless restarts. Then the system may shut down.</p>
							<p>Many Erlang developers end up arguing in favor of a supervisor that has a cooldown period. I strongly oppose the sentiment for one simple reason: <span class="it">it’s all about the guarantees</span>.</p>
						<a id="2_2_1_its_about_the_guarantees"><h4>2.2.1 It’s About the Guarantees</h4></a>
							<p>Restarting a process is about bringing it back to a stable, known state. From there, things can be retried. When the initialization isn’t stable, supervision is worth very little. An initialized process should be stable no matter what happens. That way, when its siblings and cousins get started later on, they can be booted fully knowing that the rest of the system that came up before them is healthy.</p>
							<sub>
								<p><a id="sub_2_7"> 7</a> <a href="http://mononcqc.tumblr.com/post/35165909365/why-do-computers-stop">http://mononcqc.tumblr.com/post/35165909365/why-do-computers-stop</a></p>
							</sub>
							<p>If you don’t provide that stable state, or if you were to start the entire system asynchronously, you would get very little benefit from this structure that a <span class="code">try ... catch</span> in a loop wouldn’t provide.</p>
							<p>Supervised processes <span class="it">provide guarantees</span> in their initialization phase, not a <span class="it">best effort</span>. This means that when you’re writing a client for a database or service, you shouldn’t need a connection to be established as part of the initialization phase unless you’re ready to say it will always be available no matter what happens.</p>
							<p>You could force a connection during initialization if you know the database is on the same host and should be booted before your Erlang system, for example. Then a restart should work. In case of something incomprehensible and unexpected that breaks these guarantees, the node will end up crashing, which is desirable: a pre-condition to starting your system hasn’t been met. It’s a system-wide assertion that failed.</p>
							<p>If, on the other hand, your database is on a remote host, you should expect the connection to fail. It’s just a reality of distributed systems that things go down.<sup><a href="#sub_2_8">8</a></sup> In this case, the only guarantee you can make in the client process is that your client will be able to handle requests, but not that it will communicate to the database. It could return <span class="code">{error, not_connected}</span> on all calls during a net split, for example.</p>
							<p>The reconnection to the database can then be done using whatever cooldown or backoff strategy you believe is optimal, without impacting the stability of the system. It can be attempted in the initialization phase as an optimization, but the process should be able to reconnect later on if anything ever disconnects.</p>
							<p>If you expect failure to happen on an external service, do not make its presence a guarantee of your system. We’re dealing with the real world here, and failure of external dependencies is always an option.</p>
						<a id="2_2_2_side_effects"><h4>2.2.2 Side Effects</h4></a>
							<p>Of course, the libraries and processes that call such a client will then error out if they don’t expect to work without a database. That’s an entirely different issue in a different problem space, one that depends on your business rules and what you can or can’t do to a client, but one that is possible to work around. For example, consider a client for a service that stores operational metrics — the code that calls that client could very well ignore the errors without adverse effects to the system as a whole.</p>
							<p>The difference in both initialization and supervision approaches is that the client’s callers make the decision about how much failure they can tolerate, not the client itself. That’s a very important distinction when it comes to designing fault-tolerant systems. Yes, supervisors are about restarts, but they should be about restarts to a stable known state.</p>
							<sub><p><a id="sub_2_8">8</a> Or latency shoots up enough that it is impossible to tell the difference from failure.</p></sub>
						<a id="2_2_3_example_initializing"> <h4>2.2.3 Example: Initializing without guaranteeing connections</h4></a>
							<p>The following code attempts to guarantee a connection as part of the process’ state:</p>
<pre>
<code class="erlang">1 init(Args) ->
2 Opts = parse_args(Args),
3 {ok, Port} = connect(Opts),
4 {ok, #state{sock=Port, opts=Opts}}.
5
6 [...]
7
8 handle_info(reconnect, S = #state{sock=undefined, opts=Opts}) ->
9 %% try reconnecting in a loop
10 case connect(Opts) of
11 {ok, New} -> {noreply, S#state{sock=New}};
12 _ -> self() ! reconnect, {noreply, S}
13 	end;</code></pre>
						<p>Instead, consider rewriting it as:</p>
<pre>
<code class="erlang">1 init(Args) ->
2 Opts = parse_args(Args),
3 %% you could try connecting here anyway, for a best
4 %% effort thing, but be ready to not have a connection.
5 self() ! reconnect,
6 {ok, #state{sock=undefined, opts=Opts}}.
7
8 [...]
9
10 handle_info(reconnect, S = #state{sock=undefined, opts=Opts}) ->
11 %% try reconnecting in a loop
12 case connect(Opts) of
13 {ok, New} -> {noreply, S#state{sock=New}};
14 _ -> self() ! reconnect, {noreply, S}
15 	end;</code></pre>
						<p>You now allow initializations with fewer guarantees: they went from <span class="it">the connection is available</span> to <span class="it">the connection manager</span> is available.</p>
						<a id="2_2_4_in_a_nutshell"> <h4>In a nutshell</h4></a>
						<p>Production systems I have worked with have been a mix of both approaches. Things like configuration files, access to the file system (say for logging purposes), local resources that can be depended on (opening UDP ports for logs), restoring a stable state from disk or network, and so on, are things I’ll put into requirements of a supervisor and may decide to synchronously load no matter how long it takes (some applications may just end up having over 10 minute boot times in rare cases, but that’s okay because we’re possibly syncing gigabytes that we <span class="it">need</span> to work with as a base state if we don’t want to serve incorrect information.)</p>
						<p>On the other hand, code that depends on non-local databases and external services will adopt partial startups with quicker supervision tree booting because if the failure is expected to happen often during regular operations, then there’s no difference between now and later. You have to handle it the same, and for these parts of the system, far less strict guarantees are often the better solution.</p>
						<a id="2_2_5_application_strategies"> <h4>2.2.5 Application Strategies</h4></a>
						<p>No matter what, a sequence of failures is not a death sentence for the node. Once a system has been divided into various OTP applications, it becomes possible to choose which applications are vital or not to the node. Each OTP application can be started in 3 ways: temporary, transient, permanent, either by doing it manually in <span class="code">application:start(Name, Type)</span>, or in the config file for your release:</p>
						<ul>
							<li><span class="code">permanent</span>: if the app terminates, the entire system is taken down, excluding manual termination of the app with <span class="code">application:stop/1</span></li>
							<li><span class="code">transient</span>: if the app terminates for reason normal, that’s ok. Any other reason for termination shuts down the entire system.</li>
							<li><span class="code">temporary</span>: the application is allowed to stop for any reason. It will be reported, but nothing bad will happen.</li>
						</ul>
						<p>It is also possible to start an application as an <span class="it">included application</span>, which starts it under your own OTP supervisor with its own strategy to restart it.</p>
						<a id="2_3_exercises"> <h3>2.3 Exercises</h3></a>
						<h4>Review Questions</h4>
						<ol>
							<li>Are Erlang supervision trees started depth-first? breadth-first? Synchronously or asynchronously?</li>
							<li>What are the three application strategies? What do they do?</li>
							<li>What are the main differences between the directory structure of an app and a release?</li>
							<li>When should you use a release?</li>
							<li>Give two examples of the type of state that can go in a process’ init function, and two examples of the type of state that shouldn’t go in a process’ init function</li>
						</ol>
						<h4>Hands-On</h4>
						<p>Using the code at <a href="https://github.com/ferd/recon_demo">https://github.com/ferd/recon_demo</a>:</p>
						<ol>
							<li>Extract the main application hosted in the release to make it independent, and includable in other projects.</li>
							<li>Host the application somewhere (Github, Bitbucket, local server), and build a release with that application as a dependency.</li>
							<li>The main application’s workers (<span class="code">council_member</span>) starts a server and connects to it in its <span class="code">init/1</span> function. Can you make this connection happen outside of the init function’s? Is there a benefit to doing so in this specific case?</li>
						</ol>
						<a href="chapter_3"> <h2>Chapter 3</h2></a>
						<h2>Planning for Overload</h2>
						<p>By far, the most common cause of failure I’ve encountered in real-world scenarios is due	to the node running out of memory. Furthermore, it is usually related to message queues going out of bounds. <sup><a href="#sub_3_1">1</a></sup> There are plenty of ways to deal with this, but knowing which one to use will require a decent understanding of the system you’re working on.</p>
						<p>To oversimplify things, most of the projects I end up working on can be visualized as a very large bathroom sink. User and data input are flowing from the faucet. The Erlang system itself is the sink and the pipes, and wherever the output goes (whether it’s a database, an external API or service, and so on) is the sewer system.</p>
						<img src="images/sink.png">
						<p>When an Erlang node dies because of a queue overflowing, figuring out who to blame is crucial. Did someone put too much water in the sink? Are the sewer systems backing up ? Did you just design too small a pipe?</p>
						
						<sub>
							<p><a id="sub_3_1">1</a> Figuring out that a message queue is the problem is explained in <a href="#chapter_6">Chapter 6</a>, specifically in <a href="6_2_full_mailboxes">Section 6.2</a></p>
						</sub>
						<p>Determining what queue blew up is not necessarily hard. This is information that can be found in a crash dump. Finding out why it blew up is trickier. Based on the role of the process or run-time inspection, it’s possible to figure out whether causes include fast flooding, blocked processes that won’t process messages fast enough, and so on.</p>
						<p>The most difficult part is to decide how to fix it. When the sink gets clogged up by too much waste, we will usually start by trying to make the bathroom sink itself larger (the part of our program that crashed, at the edge). Then we figure out the sink’s drain is too small, and optimize that. Then we find out the pipes themselves are too narrow, and optimize that. The overload gets pushed further down the system, until the sewers can’t take it anymore. At that point, we may try to add sinks or add bathrooms to help with the global input level.</p>
						<p>Then there’s a point where things can’t be improved anymore at the bathroom’s level. There are too many logs sent around, there’s a bottleneck on databases that <span class="it">need</span> the consistency, or there’s simply not enough knowledge or manpower in your organization to improve things there.</p>
						<p>By finding that point, we identified what the <span class="it">true bottleneck</span> of the system was, and all the prior optimization was nice (and likely expensive), but it was more or less in vain.</p>
						<p>We need to be more clever, and so things are moved back up a level. We try to massage the information going in the system to make it either lighter (whether it is through compression, better algorithms and data representation, caching, and so on).</p>
						<p>Even then, there are times where the overload will be too much, and we have to make the 	hard decisions between restricting the input to the system, discarding it, or accepting that the system will reduce its quality of service up to the point it will crash. These mechanisms 	fall into two broad strategies: back-pressure and load-shedding.</p>
						<p>We’ll explore them in this chapter, along with common events that end up causing Erlang systems to blow up.</p>
						<a id="3_1_common_overload_sources"><h3>3.1 Common Overload Sources</h3></a>
						<p>There are a few common causes of queues blowing up and overload in Erlang systems that most people will encounter sooner or later, no matter how they approach their system. They’re usually symptomatic of having your system grow up and require some help scaling up, or of an unexpected type of failure that ends up cascading much harder than it should have.</p>
						<a id="3_1_1_error_logger_explodes"></a> <h4>3.1.1. error_logger Explodes</h4>
						<p>Ironically, the process in charge of error logging is one of the most fragile ones. In a defaultErlang install, the <span class="code">error_logger</span> <sup><a href="#sub_3_2">2</a><sup> process will take its sweet time to log things to disk orover the network, and will do so much more slowly than errors can be generated.</p>
						<p>This is especially true of user-generated log messages (not for errors), and for crashes in large processes. For the former, this is because <span class="code">error_logger</span> doesn’t really expect arbitrary levels of messages coming in continually. It’s for exceptional cases only and doesn’t expect lots of traffic. For the latter, it’s because the entire state of processes (including their mailboxes) gets copied over to be logged. It only takes a few messages to cause memory to bubble up a lot, and if that’s not enough to cause the node to run Out Of Memory (OOM), it may slow the logger enough that additional messages will.</p>
						<p>The best solution for this at the time of writing is to use <span class="code">lager</span> as a substitute logging library.</p>
						<p>While <span class="code">lager</span> will not solve all your problems, it will truncate voluminous log messages, optionally drop OTP-generated error messages when they go over a certain threshold, and will automatically switch between asynchronous and synchronous modes for user-submitted messages in order to self-regulate.</p>
						<p>It won’t be able to deal with very specific cases, such as when user-submitted messages are very large in volume and all coming from one-off processes. This is, however, a much rarer occurrence than everything else, and one where the programmer tends to have more control.</p>
						<a id="3_1_2_locks_and_blocking_operations"> <h3>3.1.2 Locks and Blocking Operations</h3></a>
						
						<p>Locking and blocking operations will often be problematic when they’re taking unexpectedly long to execute in a process that’s constantly receiving new tasks.</p>
						<p>One of the most common examples I’ve seen is a process blocking while accepting a connection or waiting for messages with TCP sockets. During blocking operations of this kind, messages are free to pile up in the message queue.</p>
						<p>One particularly bad example was in a pool manager for HTTP connections that I had written in a fork of the <span class="code">lhttpc</span> library. It worked fine in most test cases we had, and we even had a connection timeout set to 10 milliseconds to be sure it never took too long <sup><a href="#sub_3_3">3</a></sup>. After a few weeks of perfect uptime, the HTTP client pool caused an outage when one of the remote servers went down.</p>
						<p>The reason behind this degradation was that when the remote server would go down, all of a sudden, all connecting operations would take at least 10 milliseconds, the time before which the connection attempt is given up on. With around 9,000 messages per second to the central process, each usually taking under 5 milliseconds, the impact became similar to roughly 18,000 messages a second and things got out of hand.</p>
						<sub>
						<p><a id="sub_3_2">2</a> Defined at <a href="http://www.erlang.org/doc/man/error_logger.html">http://www.erlang.org/doc/man/error_logger.html</a></p>
						
						<p><a id="sub_3_3">3</a> 10 milliseconds is very short, but was fine for collocated servers used for real-time bidding.</p>
						</sub>
						<p>The solution we came up with was to leave the task of connecting to the caller process, and enforce the limits as if the manager had done it on its own. The blocking operations were now distributed to all users of the library, and even less work was required to be done by the manager, now free to accept more requests.</p>
						<p>When there is <span class="it">any</span> point of your program that ends up being a central hub for receiving messages, lengthy tasks should be moved out of there if possible. Handling predictable overload <sup><a href="#sub_3_4">4</a></sup> situations by adding more processes — which either handle the blocking operations or instead act as a buffer while the "main" process blocks — is often a good idea.</p>
						<p>There will be increased complexity in managing more processes for activities that aren’t intrinsically concurrent, so make sure you need them before programming defensively.</p>
						<p>Another option is to transform the blocking task into an asynchronous one. If the type of work allows it, start the long-running job and keep a token that identifies it uniquely, along with the original requester you’re doing work for. When the resource is available, have it send a message back to the server with the aforementioned token. The server will eventually get the message, match the token to the requester, and answer back, without being blocked by other requests in the mean time.<sup><a href="#sub_3_5">5</a></sup></p>
						<p>This option tends to be more obscure than using many processes and can quickly devolve into callback hell, but may use fewer resources.</p>
						<a id="3_1_3_unexpected_messages"> <h4>3.1.3 Unexpected Messages</h4></a>
						<p>Messages you didn’t know about tend to be rather rare when using OTP applications. Because OTP behaviours pretty much expect you to handle anything with some clause in <span class="code">handle_info/2</span>, unexpected messages will not accumulate much.</p>
						<p>However, all kinds of OTP-compliant systems end up having processes that may not implement a behaviour, or processes that go in a non-behaviour stretch where it overtakes message handling. If you’re lucky enough, monitoring tools <sup><a href="#sub_3_6">6</a></sup> will show a constant memory increase, and inspecting for large queue sizes <sup><a href="#sub_3_7">7</a></sup> will let you find which process is at fault. You can then fix the problem by handling the messages as required.</p>
						<sub>
						<p><a id="sub_3_4">4</a> Something you know for a fact gets overloaded in production</p>
						<p><a id="sub_3_5">5</a> The <span class="code">redo</span> application is an example of a library doing this, in its <span class="code">redo_block</span> module. The [underdocumented] module turns a pipelined connection into a blocking one, but does so while maintaining pipeline aspects to the caller — this allows the caller to know that only one call failed when a timeout occurs, not all of the in-transit ones, without having the server stop accepting requests.</p>
						<p><a id="sub_3_6">6</a> See <a href="#5_1_global_view">Section 5.1</a></p>
						<p><a id="sub_3_5">7</a> See <a href="#5_2_1_processes">Subsection 5.2.1</a></p>
						</sub>
						<a id="3_2_restricting_input"> <h3>3.2 Restricting Input</h3> </a>
						
						<p>Restricting input is the simplest way to manage message queue growth in Erlang systems. It’s the simplest approach because it basically means you’re slowing the user down (applying <span class="i">back-pressure</span>), which instantly fixes the problem without any further optimization required. On the other hand, it can lead to a really crappy experience for the user.</p>
						<p>The most common way to restrict data input is to make calls to a process whose queue would grow in uncontrollable ways synchronously. By requiring a response before moving on to the next request, you will generally ensure that the direct source of the problem will be slowed down.</p>
						<p>The difficult part for this approach is that the bottleneck causing the queue to grow is usually not at the edge of the system, but deep inside it, which you find after optimizing nearly everything that came before. Such bottlenecks will often be database operations, disk operations, or some service over the network.</p>
						
						<p>This means that when you introduce synchronous behaviour deep in the system, you’ll possibly need to handle back-pressure, level by level, until you end up at the system’s edges and can tell the user, "please slow down." Developers that see this pattern will often try to put API limits per user <sup><a href="#sub_3_8">8</a></sup> on the system entry points. This is a valid approach, especially since it can guarantee a basic quality of service (QoS) to the system and allows one to allocate resources as fairly (or unfairly) as desired. </p>
						<a id="3_2_1_how_long_should_timeout_be"><h4>3.2.1 How Long Should a Time Out Be</h4></a>
						<p>What’s particularly tricky about applying back-pressure to handle overload via synchronous calls is having to determine what the typical operation should be taking in terms of time, or rather, at what point the system should time out. </p>
						<p>The best way to express the problem is that the first timer to be started will be at the edge of the system, but the critical operations will be happening deep within it. This means that the timer at the edge of the system will need to have a longer wait time that those within, unless you plan on having operations reported as timing out at the edge even though they succeeded internally. </p>
						<p>An easy way out of this is to go for infinite timeouts. Pat Helland <sup><a href="#sub_3_9">9</a></sup> has an interesting answer to this: </p>
						
						<div>Some application developers may push for no timeout and argue it is OK to wait indefinitely. I typically propose they set the timeout to 30 years. That, in turn, generates a response that I need to be reasonable and not silly. <span class="it">Why is 30 years silly but infinity is reasonable?</span> I have yet to see a messaging application that really wants to wait for an unbounded period of time. . . </div>
						
						<sub>
						<p><a id="sub_3_8">8</a> There’s a tradeoff between slowing down all requests equally or using rate-limiting, both of which are valid. Rate-limiting per user would mean you’d still need to increase capacity or lower the limits of all users when more new users hammer your system, whereas a synchronous system that indiscriminately blocks should adapt to any load with more ease, but possibly unfairly.</p>
						<p><a id="sub_3_9">9</a> Idempotence is Not a Medical Condition, April 14, 2012</p>
						</sub>
						<p>This is, ultimately, a case-by-case issue. In many cases, it may be more practical to use a different mechanism for that flow control.<sup><a href="#sub_3_10">10</a></sup> </p>
						<a id="3_2_2_asking_for_permission"><h4>3.2.2 Asking For Permission</h4></a>
						<p>A somewhat simpler approach to back-pressure is to identify the resources we want to block on, those that cannot be made faster and are critical to your business and users. Lock these resources behind a module or procedure where a caller must ask for the right to make a request and use them. There’s plenty of variables that can be used: memory, CPU, overall load, a bounded number of calls, concurrency, response times, a combination of them, and so on.</p>
						
						<p>The <span class="it">SafetyValve</span> <sup><a href="sub_3_11">11</a></sup> application is a system-wide framework that can be used when you know back-pressure is what you’ll need. </p>
						<p>For more specific use cases having to do with service or system failures, there are plenty of circuit breaker applications available. Examples include <span class="code">breaky</span><sup><a href="sub_3_12">12</a></sup> , <span class="code">fuse</span> <sup><a href="sub_3_13">13</a></sup>, or Klarna’s <span class="code">circuit_breaker</span> <sup><a href="sub_3_14">14</a></sup>.</p>
						<p>Otherwise, ad-hoc solutions can be written using processes, ETS, or any other tool available. The important part is that the edge of the system (or subsystem) may block and ask for the right to process data, but the critical bottleneck in code is the one to determine whether that right can be granted or not. </p>
						<p>The advantage of proceeding that way is that you may just avoid all the tricky stuff about timers and making every single layer of abstraction synchronous. You’ll instead put guards at the bottleneck and at a given edge or control point, and everything in between can be expressed in the most readable way possible.</p>
						<a id="3_2_3_what_users_see"> <h4>3.2.2 What Users See</h4></a>
						<p>The tricky part about back-pressure is reporting it. When back-pressure is done implicitly through synchronous calls, the only way to know it is at work due to overload is that the system becomes slower and less usable. Sadly, this is also going to be a potential symptom of bad hardware, bad network, unrelated overload, and possibly a slow client.</p>
						<p>Trying to figure out that a system is applying back-pressure by measuring its responsiveness is equivalent to trying to diagnose which illness someone has by observing that person has a fever. It tells you something is wrong, but not what.</p>
						<sub><p>10 In Erlang, using the value <span class="code">infinity</span> will avoid 	creating a timer, avoiding some resources. If you do use this, remember to at least have a well-defined timeout somewhere in the sequence of calls.</p>
						<p><a id="sub_3_11">11</a>	<a href="https://github.com/jlouis/safetyvalve">https://github.com/jlouis/safetyvalve</a></p>
						<p><a id="sub_3_11">12</a>	<a href="https://github.com/mmzeeman/breaky">https://github.com/mmzeeman/breaky</a></p>
						<p><a id="sub_3_11">13</a>	<a href="https://github.com/jlouis/fuse">https://github.com/jlouis/fuse</a></p>
						<p><a id="sub_3_11">14</a> <a href="https://github.com/klarna/circuit_breaker">https://github.com/klarna/circuit_breaker</a></p>
						</sub>
						<p>Asking for permission, as a mechanism, will generally allow you to define your interface in such a way that you can explicitly report what is going on: the system as a whole is overloaded, or you’re hitting a limit into the rate at which you can perform an operation and adjust accordingly. </p>
						
						<p>There is a choice to be made when designing the system. Are your users going to have per-account limits, or are the limits going to be global to the system?</p>
						
						<p>System-global or node-global limits are usually easy to implement, but will have the downside that they may be unfair. A user doing 90% of all your requests may end up making the platform unusable for the vast majority of the other users.</p>
						
						<p>Per-account limits, on the other hand, tend to be very fair, and allow fancy schemes such as having premium users who can go above the usual limits. This is extremely nice, but has the downside that the more users use your system, the higher the effective global system limit tends to move. Starting with 100 users that can do 100 requests a minute gives you a global 10000 requests per minute. Add 20 new users with that same rate allowed, and suddenly you may crash a lot more often.</p>
						
						<p>The safe margin of error you established when designing the system slowly erodes as more people use it. It’s important to consider the tradeoffs your business can tolerate from that point of view, because users will tend not to appreciate seeing their allowed usage go down all the time, possibly even more so than seeing the system go down entirely from time to time.</p>
						
						<a id="3_3_discarding_data"><h3>3.3 Discarding Data</h3></a>
						<p>When nothing can slow down outside of your Erlang system and things can’t be scaled up, you must either drop data or crash (which drops data that was in flight, for most cases, but with more violence).</p>
						<p>It’s a sad reality that nobody really wants to deal with. Programmers, software engineers, and computer scientists are trained to purge the useless data, and keep everything that’s useful. Success comes through optimization, not giving up.</p>
						<p>However, there’s a point that can be reached where the data that comes in does so at a rate faster than it goes out, even if the Erlang system on its own is able to do everything fast enough. In some cases, It’s the component <span class="it">after</span> it that blocks. </p>
						<p>If you don’t have the option of limiting how much data you receive, you then have to drop messages to avoid crashing.</p>
						<a id="3_3_1_random_drop"><h4>3.3.1 Random Drop</h4></a>
						<p>Randomly dropping messages is the easiest way to do such a thing, and might also be the most robust implementation, due to its simplicity.</p>
						<p>The trick is to define some threshold value between 0.0 and 1.0 and to fetch a random number in that range:</p>
<pre><code class="erlang">-module(drop).
-export([random/1]).
random(Rate) ->
maybe_seed(),
random:uniform() =< Rate.
maybe_seed() ->
case get(random_seed) of
undefined -> random:seed(erlang:now());
{X,X,X} -> random:seed(erlang:now());
_ -> ok
end.</code></pre>
						<p>If you aim to keep 95% of the messages you send, the authorization could be written by a call to <span class="code">case drop:random(0.95) of true -> send(); false -> drop() end</span>, or a shorter <span class="code">drop:random(0.95) andalso send()</span> if you don’t need to do anything specific when dropping a message.</p>
						
						<p>The <span class="code">maybe_seed()</span> function will check that a valid seed is present in the process dictionary and use it rather than a crappy one, but only if it has not been defined before, in order to avoid calling <span class="code">now()</span> (a monotonic function that requires a global lock) too often.</p>
						
						<p>There is one ‘gotcha’ to this method, though: the random drop must ideally be done at the producer level rather than at the queue (the receiver) level. The best way to avoid overloading a queue is to not send data its way in the first place. Because there are no bounded mailboxes in Erlang, dropping in the receiving process only guarantees that this process will be spinning wildly, trying to get rid of messages, and fighting the schedulers to do actual work.</p>
						<p>On the other hand, dropping at the producer level is guaranteed to distribute the work equally across all processes.</p>
						
						<p>This can give place to interesting optimizations where the working process or a given monitor process <sup><a href="#sub_3_15">15</a></sup> uses values in an ETS table or <span class="code">application:set_env/3</span> to dynamically increase and decrease the threshold to be used with the random number. This allows control over how many messages are dropped based on overload, and the configuration data can be fetched by any process rather efficiently by using <span class="code">application:get_env/2</span>.</p>
						
						<p>Similar techniques could also be used to implement different drop ratios for different message priorities, rather than trying to sort it all out at the consumer level.</p>
						
						<sub><p><a id="sub_3_15">15</a>
							Any process tasked with checking the load of specific processes using heuristics such as <span class="code">process_info(Pid, message_queue_len)</span> could be a monitor</p>
						</sub>
						<a id="3_3_2_queue_buffers"> <h4>3.3.2 Queue Buffers</h4> </a>
						<p>Queue buffers are a good alternative when you want more control over the messages you get rid of than with random drops, particularly when you expect overload to be coming in bursts rather than a constant stream in need of thinning.</p>
						<p>Even though the regular mailbox for a process has the form of a queue, you’ll generally want to pull <span class="it">all</span> the messages out of it as soon as possible. A queue buffer will need two processes to be safe: </p>
						
						<ul>
							<li>The regular process you’d work with (likely a <span class="code">gen_server</span>);</li>
							<li>A new process that will do nothing but buffer the messages. Messages from the outside should go to this process.</li>
						</ul>
						
						<p>To make things work, the buffer process only has to remove all the messages it can from its mail box and put them in a queue data structure <sup><a href="#sub_3_16">16</a></sup> it manages on its own. Whenever the server is ready to do more work, it can ask the buffer process to send it a given number of messages that it can work on. The buffer process picks them from its queue, forwards them to the server, and goes back to accumulating data. </p>
						
						<p>Whenever the queue grows beyond a certain size <sup><a href="#sub_3_17">17</a></sup> and you receive a new message, you can then pop the oldest one and push the new one in there, dropping the oldest elements as you go. <sup><a href="#sub_3_18">18</a></sup> </p>
						
						<p>This should keep the entire number of messages received to a rather stable size and provide a good amount of resistance to overload, somewhat similar to the functional version of a ring buffer.</p>
						
						<p>The <span class="code">PO Box</span> <sup><a href="#sub_3_19">19</a></sup> library implements such a queue buffer.</p>
						
						<a id="3_3_3_stack_buffers"> <h4>3.3.3 Stack Buffers </h4></a>
						<p>Stack buffers are ideal when you want the amount of control offered by queue buffers, but you have an important requirement for low latency.</p>
						
						<p>To use a stack as a buffer, you’ll need two processes, just like you would with queue buffers, but a list <sup><a href="#sub_3_20">20</a></sup> will be used instead of a queue data structure.</p>
						
						<sub>
						<p><a id="sub_3_16">16</a>	The <span class="code">queue</span> module in Erlang provides a purely functional queue data structure that can work fine for such a buffer.</p>
						<p><a id="sub_3_17">17</a> To calculate the length of a queue, it is preferable to use a counter that gets incremented and decremented on each message sent or received, rather than iterating over the queue every time. It takes slightly more memory, but will tend to distribute the load of counting more evenly, helping predictability and avoiding more sudden build-ups in the buffer’s mailbox</p>
						<p><a id="sub_3_18">18</a> You can alternatively make a queue that pops the newest message and queues up the oldest ones if you feel previous data is more important to keep.</p>
						<p><a id="sub_3_19">19</a> Available at: <a href="https://github.com/ferd/pobox">https://github.com/ferd/pobox</a>, the library has been used in production for a long time in large scale products at Heroku and is considered mature</p>
						<p><a id="sub_3_20">20</a> Erlang lists <span class="it">are stacks</span>, for all we care. They provide push and pop operations that take O(1) complexity and are very fast</p>
						</sub>
						<p>The reason the stack buffer is particularly good for low latency is related to issues similar to bufferbloat <sup><a href="#sub_3_21">21</a></sup>. If you get behind on a few messages being buffered in a queue, all the messages in the queue get to be slowed down and acquire milliseconds of wait time. Eventually, they all get to be too old and the entire buffer needs to be discarded.</p>
						
						<p>On the other hand, a stack will make it so only a restricted number of elements are kept waiting while the newer ones keep making it to the server to be processed in a timely manner.</p>
						
						<p>Whenever you see the stack grow beyond a certain size or notice that an element in it is too old for your QoS requirements you can just drop the rest of the stack and keep going from there. <span class="code">PO Box</span> also offers such a buffer implementation. </p>
						<p>A major downside of stack buffers is that messages are not necessarily going to be processed in the order they were submitted — they’re nicer for independent tasks, but will ruin your day if you expect a sequence of events to be respected. </p>
						
						<a id="3_3_4_time_sensitive_buffers"><h4>3.3.4 Time-Sensitive Buffers</h4></a>
						<p>If you need to react to old events <span class="it">before</span> they are too old, then things become more complex, as you can’t know about it without looking deep in the stack each time, and dropping from the bottom of the stack in a constant manner gets to be inefficient. An interesting approach could be done with buckets, where multiple stacks are used, with each of them containing a given time slice. When requests get too old for the QoS constraints, drop an entire bucket, but not the entire buffer.</p>
						
						<p>It may sound counter-intuitive to make some requests a lot worse to benefit the majority — you’ll have great medians but poor 99 percentiles — but this happens in a state where you would drop messages anyway, and is preferable in cases where you do need low latency. </p>
						<a id="3_3_5_dealing_with_constant_overload"> <h4>3.3.5 Dealing With Constant Overload</h4></a>
						<p>Being under constant overload may require a new solution. Whereas both queues and buffers will be great for cases where overload happens from time to time (even if it’s a rather prolonged period of time), they both work more reliably when you expect the input rate to eventually drop, letting you catch up.</p>
						
						<p>You’ll mostly get problems when trying to send so many messages they can’t make it all to one process without overloading it. Two approaches are generally good for this case: </p>
						
						<ul>
							<li>Have many processes that act as buffers and load-balance through them (scale horizontally)</li>
							<li>use ETS tables as locks and counters (reduce the input)</li>
						</ul>
						
						<p>ETS tables are generally able to handle a ton more requests per second than a process, but the operations they support are a lot more basic. A single read, or adding or removing from a counter atomically is as fancy as you should expect things to get for the general case.</p>
						
						<sub><a id="sub_3_21">21</a> <a href="http://queue.acm.org/detail.cfm?id=2071893">http://queue.acm.org/detail.cfm?id=2071893</a></sub>
						
						<p>ETS tables will be required for both approaches.</p>
						
						<p>Generally speaking, the first approach could work well with the regular process registry: you take N processes to divide up the load, give them all a known name, and pick one of them to send the message to. Given you’re pretty much going to assume you’ll be overloaded, randomly picking a process with an even distribution tends to be reliable: no state communication is required, work will be shared in a roughly equal manner, and it’srather insensitive to failure. </p>
						
						<p>In practice, though, we want to avoid atoms generated dynamically, so I tend to prefer to register workers in an ETS table with <span class="code">read_concurrency</span> set to <span class="code">true</span>. It’s a bit more work, but it gives more flexibility when it comes to updating the number of workers later on.</p>
						
						<p>An approach similar to this one is used in the <span class="code">lhttpc</span> <sup><a href="#sub_3_22">22</a></sup> library mentioned earlier, to split load balancers on a per-domain basis. </p>
						
						<p>For the second approach, using counters and locks, the same basic structure still remains (pick one of many options, send it a message), but before actually sending a message, you must atomically update an ETS counter <sup><a href="#sub_3_23">23</a></sup> . There is a known limit shared across all clients (either through their supervisor, or any other config or ETS value) and each request that can be made to a process needs to clear this limit first.</p>
						
						<p>This approach has been used in <span class="code">dispcount</span> <sup><a href="#sub_3_24">24</a></sup> to avoid message queues, and to guarantee low-latency responses to any message that won’t be handled so that you do not need to wait to know your request was denied. It is then up to the user of the library whether to give up as soon as possible, or to keep retrying with different workers. </p>
						<a id="3_3_6_how_to_drop"><h4>3.3.6 How Do You Drop</h4></a>
						<p>Most of the solutions outlined here work based on message quantity, but it’s also possible to try and do it based on message size, or expected complexity, if you can predict it. When using a queue or stack buffer, instead of counting entries, all you may need to do is count their size or assign them a given load as a limit. </p>
						<p>I’ve found that in practice, dropping without regard to the specifics of the message works rather well, but each application has its share of unique compromises that can be acceptable or not <sup><a href="#sub_3_25">25</a></sup>. </p>
						<sub>
						<p><a id="sub_3_22">22</a> The <a href="https://github.com/ferd/lhttpc/blob/master/src/lhttpc_lb.erl">lhttpc_lb</a> module in this library implements it.</p>
						
						<p><a id="sub_3_23">23</a> By using <span class="code">ets:update_counter/3</span>.</p>
						
						<p><a id="sub_3_24">24</a> <a href="https://github.com/ferd/dispcount">https://github.com/ferd/dispcount</a></p>
						<p><a id="sub_3_25">25</a> Old papers such as <a href="http://research.microsoft.com/en-us/um/people/blampson/33-hints/webpage.html">Hints for Computer System Designs</a> by Butler W. Lampson recommend dropping messages: "Shed load to control demand, rather than allowing the system to become overloaded." The paper also mentions that "A system cannot be expected to function well if the demand for any resource exceeds two-thirds of the capacity, unless the load can be characterized extremely well." adding that "The only systems in which cleverness has worked are those with very well-known loads."</p>
						</sub>
						
						<p>There are also cases where the data is sent to you in a "fire and forget" manner — the entire system is part of an asynchronous pipeline — and it proves difficult to provide feedback to the end-user about why some requests were dropped or are missing. If you can reserve a special type of message that accumulates dropped responses and tells the user "N messages were dropped for reason X", that can, on its own, make the compromise far more acceptable to the user. This is the choice that was made with Heroku’s <a href="https://devcenter.heroku.com/articles/logplex"><span class="code">logplex</span></a> log routing system, which can spit out <a href="https://devcenter.heroku.com/articles/error-codes#l10-drain-buffer-overflow"><span class="code">L10 errors</span></a>, alerting the user that a part of the system can’t deal with all the volume right now.</p>
						
						<p>In the end, what is acceptable or not to deal with overload tends to depend on the humans that use the system. It is often easier to bend the requirements a bit than develop new technology, but sometimes it is just not avoidable. </p>
						<a id="3_4_exercises"><h3>3.4 Exercises</h3></a>
						<h4>Review Questions</h4>
						<ol>
							<li>Name the common sources of overload in Erlang systems</li>
							<li>What are the two main classes of strategies to handle overload?</li>
							<li>How can long-running operations be made safer?</li>
							<li>When going synchronous, how should timeouts be chosen?</li>
							<li>What is an alternative to having timeouts?</li>
							<li>When would you pick a queue buffer before a stack buffer?</li>
						</ol>
						
						<h4>Open-ended Questions</h4>
						<ol>
							<li>What is a <span class="it">true bottleneck</span> ? How can you find it? In an application that calls a third party API, response times vary by a lot depending on how healthy the other servers are. How could one design the system to prevent occasionally slow requests from blocking other concurrent calls to the same service? </li>
							<li>What’s likely to happen to new requests to an overloaded latency-sensitive service where data has backed up in a stack buffer? What about old requests? </li>
							<li>Explain how you could turn a load-shedding overload mechanism into one that can also provide back-pressure. </li>
							<li>Explain how you could turn a back-pressure mechanism into a load-shedding mechanism.</li>
							<li>What are the risks, for a user, when dropping or blocking a request? How can we prevent duplicate messages or missed ones?</li>
							<li>What can you expect to happen to your API design if you forget to deal with overload, and suddenly need to add back-pressure or load-shedding to it?</li>
						</ol>
		<a id="part_ii"><h1>Part II</h1></a>
		<h1>Diagnosing Applications</h1>
			<a id="chapter_4"><h2>Chapter 4</h2> </a>
			<h2>Connecting to Remote Nodes</h2>
				<p>Interacting with a running server program is traditionally done in one of two ways. One is to do it through an interactive shell kept available by using a <span class="code">screen</span> or <span class="code">tmux</span> session that runs in the background and letting someone connect to it. The other is to program management functions or comprehensive configuration files that can be dynamically reloaded.</p>
						
				<p>The interactive session approach is usually okay for software that runs in a strict Read-Eval-Print-Loop (REPL). The programmed management and configuration approach requires careful planning in whatever tasks you think you’ll need to do, and hopefully getting it right. Pretty much all systems can try that approach, so I’ll skip it given I’m somewhat more interested in the cases where stuff is already bad and no function exists for it.</p>
						
				<p>Erlang uses something closer to an "interactor" than a REPL. Basically, a regular Erlang virtual machine does not need a REPL, and will happily run byte code and stick with that, no shell needed. However, because of how it works with concurrency and multiprocessing, and good support for distribution, it is possible to have in-software REPLs that run as arbitrary Erlang processes.</p>
						
				<p>This means that, unlike a single screen session with a single shell, it’s possible to have as many Erlang shells connected and interacting with one virtual machine as you want at a time <sup><a href="#sub_4_1">1</a></sup>.</p>
				<p>Most common usages will depend on a cookie being present on the two nodes you want to connect together <sup><a href="#sub_4_2">2</a></sup>, but there are ways to do it that do not include it. Most usages will also require the use of named nodes, and all of them will require <span class="it">a priori</span> measures to make sure you can contact the node.</p>
				<sub>
					<p><a id="sub_4_1">1</a> More details on the mechanisms at <a href="http://ferd.ca/repl-a-bit-more-and-less-than-that.html">http://ferd.ca/repl-a-bit-more-and-less-than-that.html</a></p>
					<p><a id="sub_4_2">2</a> More details at  <a href="http://learnyousomeerlang.com/distribunomicon#cookies">http://learnyousomeerlang.com/distribunomicon#cookies</a> <a href="http://www.erlang.org/doc/reference_manual/distributed.html#id83619">http://www.erlang.org/doc/reference_manual/distributed.html#id83619</a></p>
				</sub>
				<a id="4_1_job_control_mode"><h3>4.1 Job Control Mode </h3></a>
						
					<p>The Job Control Mode (JCL mode) is the menu you get when you press <span class="code">ˆG</span> in the Erlang shell. From that menu, there is an option allowing you to connect to a remote shell:</p>
<pre>
<code class="erlang">(somenode@ferdmbp.local)1>
User switch command
--> h
c [nn]  - connect to job
i [nn]  - interrupt job
k [nn]  - kill job
j  - list all jobs
s [shell]  - start local shell
r [node [shell]]  - start remote shell
q  - quit erlang
? | h    - this message
--> r ’server@ferdmbp.local’
--> c
Eshell Vx.x.x (abort with ^G)
(server@ferdmbp.local)1></code></pre>
					<p>When that happens, the local shell runs all the line editing and job management locally, but the evaluation is actually done remotely. All output coming from said remote evaluation will be forwarded to the local shell.</p>
					<p>To quit the shell, go back in the JCL mode with <span class="code">ˆG</span>. This job management is, as I said, done locally, and it is thus safe to quit with <span class="code">ˆG q</span>:</p>
<pre>
<code class="erlang">(server@ferdmbp.local)1>
User switch command
--> q
</code>
</pre>
					<p>You may choose to start the initial shell in hidden mode (with the argument <span class="code">-hidden</span>) to avoid connecting to an entire cluster automatically. </p>
			<a id="4_2_remsh"><h3>4.2 Remsh</h3></a>
				<p>There’s a mechanism entirely similar to the one available through the JCL mode, although	invoked in a different manner. The entire JCL mode sequence can by bypassed by starting	the shell as follows for long names: </p>
<pre><code class="erlang">erl -name local@domain.name -remsh remote@domain.name
</code></pre>
				<p>And as follows for short names:</p>
						
<pre><code class="erlang">erl -sname local@domain -remsh remote@domain</code></pre>
				<p>All other Erlang arguments (such as <span class="code">-hidden</span> and <span class="code">-setcookie $COOKIE</span>) are also valid. The underlying mechanisms are the same as when using JCL mode, but the initial shell is started remotely instead of locally (JCL is still local). <span class="code">ˆG</span> remains the safest way to exit the remote shell. </p>
				<a id="4_3_ssh_daemon"><h3>4.3 SSH Daemon</h3></a>
					<p>Erlang/OTP comes shipped with an SSH implementation that can both act as a server and a client. Part of it is a demo application providing a remote shell working in Erlang.</p>
					<p>To get this to work, you usually need to have your keys to have access to SSH stuff remotely in place already, but for quick test purposes, you can get things working by doing:</p>
<pre>
<code class="erlang">$ mkdir /tmp/ssh
$ ssh-keygen -t rsa -f /tmp/ssh/ssh_host_rsa_key
$ ssh-keygen -t rsa1 -f /tmp/ssh/ssh_host_key
$ ssh-keygen -t dsa -f /tmp/ssh/ssh_host_dsa_key
$ erl
1> application:ensure_all_started(ssh).
{ok,[crypto,asn1,public_key,ssh]}
2> ssh:daemon(8989, [{system_dir, "/tmp/ssh"},
2>  {user_dir, "/home/ferd/.ssh"}]).
{ok,<0.52.0>}
</code></pre>
					<p>I’ve only set a few options here, namely system_dir, which is where the host files are, and <span class="code">user_dir</span>, which contains SSH configuration files. There are plenty of other options available to allow for specific passwords, customize handling of public keys, and so on <sup><a href="#sub_4_3">3</a></sup>.</p>
						
					<p>To connect to the daemon, any SSH client will do:</p>
<pre>
<code class="erlang">$ ssh -p 8989 ferd@127.0.0.1
Eshell Vx.x.x (abort with ^G)
1>
</code></pre>
					<sub>
					<p><a id="sub_4_3">3</a>
							Complete instructions with all options to get this set up are
							available at
							<a href="http://www.erlang.org/doc/man/ssh.html#daemon-3">http://www.erlang.org/doc/man/ssh.html#daemon-3</a>.</p>
					</sub>
						
					<p>And with this you can interact with an Erlang installation without having it installed on the current machine. Just disconnecting from the SSH session (closing the terminal) will be enough to leave. <span class="it">Do not run</span> functions such as <span class="code">q()</span> or <span class="code">init:stop()</span>, which will terminate the remote host.<sup><a href="#sub_4_4">4</a></sup> </p>
						
					<p>If you have trouble connecting, you can add the <span class="code">-oLogLevel=DEBUG</span> option to <span class="code">ssh</span> to get debug output.</p>
				<a id="4_4_named_pipes"><h3>4.4. Named Pipes</h3></a>
					
					<p>A little known way to connect with an Erlang node that requires no explicit distribution is through named pipes. This can be done by starting Erlang with <span class="code">run_erl</span>, which wraps Erlang in a named pipe <sup><a href="#sub_4_5">5</a></sup> : </p>
<pre><code class="erlang">$ run_erl /tmp/erl_pipe /tmp/log_dir "erl"
</code></pre>
					<p>The first argument is the name of the file that will act as the named pipe. The second one is where logs will be saved <sup><a href="#sub_4_6">6</a></sup> .</p>
					<p>To connect to the node, you use the <span class="code">to_erl</span> program:</p>
<pre><code class="erlang">$ to_erl /tmp/erl_pipe
Attaching to /tmp/erl_pipe (^D to exit)
1>
</code></pre>
					<p>And the shell is connected. Closing stdio (with <span class="code">ˆD</span>) will disconnect from the shell while leaving it running. </p>
						
					<sub>
						<p><a id="sub_4_4">4</a> This is true for all methods of interacting with a remote Erlang node.</p>
						<p><a id="sub_4_5">5</a> <span class="code">"erl"</span> is the command being run. Additional arguments can be added after it. For example <span class="code">"erl +K true"</span> will turn kernel polling on.</p>
						<p><a id="sub_4_6">6</a> Using this method ends up calling fsync for each piece of output, which may give quite a performance hit if a lot of IO is taking place over standard output</p>
					</sub>
				<a id="4_5_exercises"> <h3>4.5 Exercises </h3> </a>
					<h4>Review Questions</h4>
						<ol>
							<li>What are the 4 ways to connect to a remote node?</li>
							<li>Can you connect to a node that wasn’t given a name?</li>
							<li>What’s the command to go into the Job Control Mode (JCL)?</li>
							<li>Which method(s) of connecting to a remote shell should you avoid for a system that outputs a lot of data to standard output?</li>
							<li>What instances of remote connections shouldn’t be disconnected using <span class="code">ˆG</span>?</li>
							<li>What command(s) should never be used to disconnect from a session?</li>
							<li>Can all of the methods mentioned support having multiple users connected onto the same Erlang node without issue?</li>
						</ol>
		<a id="chapter_5"><h2>Chapter 5</h2></a>
		<h2>RuntimeMetrics</h2>

			<p>One of the best selling points of the Erlang VM for production use is how transparent it can be for all kinds of introspection, debugging, profiling, and analysis at run time.</p>
		
			<p>The advantage of having these runtime metrics accessible programmatically is that building tools relying on them is easy, and building automation for some tasks or watchdogs is equally simple <sup><a href="#sub_5_1">1</a></sup> . Then, in times of need, it’s also possible to bypass the tools and go direct to the VM for information.</p>
		
			<p>A practical approach to growing a system and keeping it healthy in production is to make sure all angles are observable: in the large, and in the small. There’s no generic recipe to tell in advance what is going to be normal or not. You want to keep a lot of data and to look at it from time to time to form an idea about what your system looks like under normal circumstances. The day something goes awry, you will have all these angles you’ve grown to know, and it will be simpler to find what is off and needs fixing. </p>
		
			<p>For this chapter (and most of those that follow), most of the concepts or features to be shown are accessible through code in the standard library, part of the regular OTP distribution.</p>
						
			<p>However, these features aren’t all in one place, and can make it too easy to shoot yourself in the foot within a production system. They also tend to be closer to building blocks than usable tools.</p>
						
			<p>Therefore, to make the text lighter and to be more usable, common operations have been regrouped in the <span class="code">recon</span><sup><a href="#sub_5_2">2</a></sup> library, and are generally production-safe.</p>
						
						<sub>
						<a id="sub_5_1">1</a> Making sure your automated processes don’t run away and go overboard with whatever corrective actions they take is more complex
						<a id="sub_5_2">2</a> <a href="http://ferd.github.io/recon/">http://ferd.github.io/recon/</a>
						</sub>
						<a id="5_1_global_view"><h3>5.1 Global View</h3></a>
						<p>For a view of the VM in the large, it’s useful to track statistics and metrics general to the VM, regardless of the code running on it. Moreover, you should aim for a solution that allows long-term views of each metric — some problems show up as a very long accumulation over weeks that couldn’t be detected over small time windows.</p>
						<p>Good examples for issues exposed by a long-term view include memory or process leaks, but also could be regular or irregular spikes in activities relative to the time of the day or week, which can often require having months of data to be sure about it.</p>
						
						<p>For these cases, using existing Erlang metrics applications is useful. Common options are:</p>
						<ul>
							<li> <span class="code">folsom</span><sup><a href="#sub_5_3">3</a></sup> to store metrics in memory within the VM, whether global or app-specific</li>
							<li><span class="code">vmstats</span><sup><a href="#sub_5_4">4</a></sup> and <span class="code">statsderl</span><sup><a href="#sub_5_5">5</a></sup>, sending node metrics over to graphite through <span class="code">statsd</span><sup><a href="#sub_5_6">6</a></sup> .</li>
							<li><span class="code">exometer</span><sup><a href="#sub_5_7">7</a></sup>, a fancy-pants metrics system that can integrate with <span class="code">folsom</span> (among other things), and a variety of back-ends (graphite, collectd, <span class="code">statsd</span>, Riak, SNMP, etc.). It’s the newest player in town</li>
							<li><span class="code">ehmon</span><sup><a href="#sub_5_8">8</a></sup> for output done directly to standard output, to be grabbed later through specific agents, splunk, and so on.</li>
							<li>custom hand-rolled solutions, generally using ETS tables and processes periodically dumping the data.<sup><a href="#sub_5_9">9</a></sup></li>
							<li> or if you have nothing and are in trouble, a function printing stuff in a loop in a shell<sup><a href="#sub_5_10">10</a></sup>.</li>
						</ul>
						<p>It is generally a good idea to explore them a bit, pick one, and get a persistence layer that will let you look through your metrics over time. </p>
						
						<a id="5_1_1_memory"><h4>5.1.1 Memory</h4></a>
						
						<p>The memory reported by the Erlang VM in most tools will be a variant of what is reported by <span class="code">erlang:memory()</span>:</p>
<pre><code class="erlang">
1> erlang:memory().
[{total,13772400},
 {processes,4390232},
 {processes_used,4390112},
 {system,9382168},
 {atom,194289},
 {atom_used,173419},
 {binary,979264},
 {code,4026603},
 {ets,305920}]
</code></pre>
						<sub>
						<p><a id="sub_5_3">3</a> <a href="https://github.com/boundary/folsom">https://github.com/boundary/folsom</a></p>
						<p><a id="sub_5_4">4</a> <a href="https://github.com/ferd/vmstats">https://github.com/ferd/vmstats</a></p>
						<p><a id="sub_5_5">5</a> <a href="https://github.com/lpgauth/statsderl">https://github.com/lpgauth/statsderl</a></p>
						<p><a id="sub_5_6">6</a> <a href="https://github.com/etsy/statsd/">https://github.com/etsy/statsd/</a></p>
						<p><a id="sub_5_7">7</a> <a href="https://github.com/Feuerlabs/exometer">https://github.com/Feuerlabs/exometer</a></p>
						<p><a id="sub_5_8">8</a> <a href="https://github.com/heroku/ehmon">https://github.com/heroku/ehmon</a></p>
						<p><a id="sub_5_9">9</a> Common patterns may fit the <span class="code">ectr</span> application, at <a href="https://github.com/heroku/ectr">https://github.com/heroku/ectr</a></p>
						<p><a id="sub_5_10">10</a> The <span class="code">recon</span> application has the function <span class="code">recon:node_stats_print/2</span> to do this if you’re in a pinch</p>
						</sub>
						
						<p>This requires some explaining.</p>
						
						<p>First of all, all the values returned are in bytes, and they represent memory <span class="it">allocated</span> (memory actively used by the Erlang VM, not the memory set aside by the operating system for the Erlang VM). It will sooner or later look much smaller than what the operating system reports.</p>
						
						<p>The <span class="code">total</span> field contains the sum of the memory used for <span class="code">processes</span> and <span class="code">system</span> (which is incomplete, unless the VM is instrumented!). <span class="code">processes</span> is the memory used by Erlang processes, their stacks and heaps. <span class="code">system</span> is the rest: memory used by ETS tables, atoms in the VM, refc binaries<sup><a href="#sub_5_11">11</a></sup> , and some of the hidden data I mentioned was missing.</p>
						
						<p>If you want the total amount of memory owned by the virtual machine, as in the amount that will trip system limits (<span class="code">ulimit</span>), this value is more difficult to get from within the VM. If you want the data without calling <span class="code">top</span> or <span class="code">htop</span>, you have to dig down into the VM’s memory allocators to find things out.<sup><a href="#sub_5_12">12</a></sup> </p>
						
						<p>Fortunately, recon has the function <span class="code">recon_alloc:memory/1</span> to figure it out, where the argument is:</p>
						
						<ul>
							<li><span class="code">used</span> reports the memory that is actively used for allocated Erlang data;</li>
							<li><span class="code">allocated</span> reports the memory that is reserved by the VM. It includes the memory used, but also the memory yet-to-be-used but still given by the OS. This is the amount you want if you’re dealing with <span class="code">ulimit</span> and OS-reported values.</li>
							<li><span class="code">unused</span> reports the amount of memory reserved by the VM that is not being allocated. Equivalent to <span class="code">allocated - used</span>.</li>
							<li><span class="code">usage</span> returns a percentage (0.0 .. 1.0) of used over allocated memory ratios.</li>
						</ul>
						<p>There are additional options available, but you’ll likely only need them when investigating memory leaks in <a href="#chapter_7">chapter 7</a></p>
						
						<a id="5_1_2_cpu"><h4>5.1.2 CPU</h4></a>
						<p>Unfortunately for Erlang developers, CPU is very hard to profile. There are a few reasons for this:</p>
						
						<sub>
						<p><a id="sub_5_11">11</a> See <a href="#7_2_binaries">Section 7.2</a></p>
						<p><a id="sub_5_12">12</a> See<a href="#7_3_2_erlang_memory_model"> Section 7.3.2</a></p>
						</sub>
						
						<ul>
							<li>The VM does a lot of work unrelated to processes when it comes to scheduling — high scheduling work and high amounts of work done by the Erlang processes are hard to characterize.</li>
							<li> The VM internally uses a model based on <span class="it">reductions</span>, which represent an arbitrary number of work actions. Every function call, including BIFs, will increment a process reduction counter. After a given number of reductions, the process gets descheduled.</li>
							<li>To avoid going to sleep when work is low, the threads that control the Erlang schedulers will do busy looping. This ensures the lowest latency possible for sudden load spikes. The VM flag <span class="code">+sbwt none|very_short|short|medium|long|very_long</span> can be used to change this value. </li>
						</ul>
						<p>These factors combine to make it fairly hard to find a good absolute measure of how busy your CPU is actually running Erlang code. It will be common for Erlang nodes in production to do a moderate amount of work and use a lot of CPU, but to actually fit a lot of work in the remaining place when the workload gets higher.</p>
						
						<p>The most accurate representation for this data is the scheduler wall time. It’s an optional metric that needs to be turned on by hand on a node, and polled at regular intervals. It will reveal the time percentage a scheduler has been running processes and normal Erlang code, NIFs, BIFs, garbage collection, and so on, versus the amount of time it has spent idling or trying to schedule processes.</p>
						
						<p>The value here represents <span class="it">scheduler utilization</span> rather than CPU utilization. The higher the ratio, the higher the workload.</p>
						
						<p>While the basic usage is explained in the Erlang/OTP reference manual<sup><a href="#sub_5_13">13</a></sup> , the value can be obtained by calling recon:</p>
<pre><code class="erlang">
1> recon:scheduler_usage(1000).
[{1,0.9919596133421669},
 {2,0.9369579039389054},
 {3,1.9294092120138725e-5},
 {4,1.2087551402238991e-5}]
</code></pre>
						<p>The function <span class="code">recon:scheduler_usage(N)</span> will poll for N milliseconds (here, 1 second) and output the value of each scheduler. In this case, the VM has two very loaded schedulers (at 99.2% and 93.7% repectively), and two mostly unused ones at far below 1%. Yet, a tool like htop would report something closer to this for each core: </p>
<pre><code class="erlang">
1 [||||||||||||||||||||||||| 70.4%]
2 [|||||||                   20.6%]
3 [|||||||||||||||||||||||||100.0%]
4 [||||||||||||||||          40.2%]
</code></pre>
						<sub>
						<p><a id="sub_5_13">13</a> <a href="http://www.erlang.org/doc/man/erlang.html#statistics_scheduler_wall_time">http://www.erlang.org/doc/man/erlang.html#statistics_scheduler_wall_time</a></p>
						</sub>
						
						<p>The result being that there is a decent chunk of CPU usage that would be mostly free for scheduling actual Erlang work (assuming the schedulers are busy waiting more than trying to select tasks to run), but is being reported as busy by the OS.</p>
						
						<p>Anotehr interesting behaviour possible is that the scheduler usage may show a higher rate (1.0) than what the OS will report. Schedulers waiting for os resources are considered utilized as they cannot handle more work. If the OS itself is holding up on non-CPU tasks it is still possible for Erlang’s schedulers not to be able to do more work and report a full ratio.</p>
						
						<p>These behaviours may especially be important to consider when doing capacity planning, and can be better indicators of headroom than looking at CPU usage or load.</p>
						
						<a id="5_1_3_processes"><h4>5.1.3 Processes</h4></a>
						<p>Trying to get a global view of processes is helpful when trying to assess how much work is being done in the VM in terms of <span class="it">tasks</span>. A general good practice in Erlang is to use processes for truly concurrent activities — on web servers, you will usually get one process per request or connection, and on stateful systems, you may add one process per-user — and therefore the number of processes on a node can be used as a metric for load.</p>
						
						<p>Most tools mentioned in section 5.1 will track them in one way or another, but if the process count needs to be done manually, calling the following expression is enough:</p>
<pre><code class="erlang">1> length(processes()).
56535
</code></pre>
						<p>Tracking this value over time can be extremely helpful to try and characterize load or detect process leaks, along with other metrics you may have around. </p>
						<a id="5_1_4_ports"<h4>5.1.14 Ports</h4>
							
							<p>In a manner similar to processes, <span class="it">Ports</span> should be considered. Ports are a datatype that encompasses all kinds of connections and sockets opened to the outside world: TCP sockets, UDP sockets, SCTP sockets, file descriptors, and so on.</p>
							
							<p>There is a general function (again, similar to processes) to count them: <span class="code">length(erlang:ports())</span>. However, this function merges in all types of ports into a single entity. Instead, one can use <span class="code">recon</span> to get them sorted by type:</p>
<pre><code class="erlang">1> recon:port_types().
[{"tcp_inet",21480},
 {"efile",2},
 {"udp_inet",2},
 {"0/1",1},
 {"2/2",1},
 {"inet_gethost 4 ",1}]
</code></pre>
							<p>This list contains the types and the count for each type of port. The type name is a string and is defined by the Erlang VM itself.</p>
							
							<p>All the <span class="code">*_inet</span> ports are usually sockets, where the prefix is the protocol used (TCP, UDP, SCTP). The <span class="code">efile</span> type is for files, while "0/1" and "2/2" are file descriptors for standard I/O channels (<span class="it">stdin</span> and <span class="code">stdout</span>) and standard error channels (<span class="code">stderr</span> ), respectively.</p>
							
							<p>Most other types will be given names of the driver they’re talking to, and will be examples of <span class="it">port programs</span> <sup><a href="#sub_5_14">14</a></sup> or <span class="it">port drivers</span> <sup><a href="#sub_5_15">15</a></sup>.</p>
							
							<p>Again, tracking these can be useful to assess load or usage of a system, detect leaks, and so on.</p>
							
							<a id="5_2_digging_in"><h3>5.2 Digging In</h3></a>
							
							<p>Whenever some ’in the large’ view (or logging, maybe) has pointed you towards a potential cause for an issue you’re having, it starts being interesting to dig around with a purpose. Is a process in a weird state? Maybe it needs tracing<sup><a href="#sub_5_16">16</a></sup> ! Tracing is great whenever you have a specific function call or input or output to watch for, but often, before getting there, a lot more digging is required. </p>
							
							<p>Outside of memory leaks, which often need their own specific techniques and are discussed in <a href="#chapter_7">Chapter 7</a>, the most common tasks are related to processes, and ports (file descriptors and sockets).</p>
							<a id="5_2_1_processes"> <h4>5.2.1 Processes</h4></a>
							<p>By all means, processes are an important part of a running Erlang system. And becausethey’re so central to everything that goes on, there’s a lot to want to know about them. Fortunately, the VM makes a lot of information available, some of which is safe to use, and some of which is unsafe to use in production (because they can return data sets large enough that the amount of memory copied to the shell process and used to print it can kill the node).</p>
							
							<sub>
							<p><a id="sub_5_14">14</a> <a href="http://www.erlang.org/doc/tutorial/c_port.html">http://www.erlang.org/doc/tutorial/c_port.html</a></p>
							<p><a id="sub_5_15">15</a> <a href="http://www.erlang.org/doc/tutorial/c_portdriver.html">http://www.erlang.org/doc/tutorial/c_portdriver.html</a></p>
							<p><a id="sub_5_16">16</a> See <a href="#chapter_9">Chapter 9</a></p>
							</sub>
							
							<p>All the values can be obtained by calling <span class="code">process_info(Pid, Key)</span> or <span class="code">process_info(Pid, [Keys])</span><sup><a href="#sub_5_17">17</a></sup> . Here are the commonly used keys<sup><a href="#sub_5_18">18</a></sup> :</p>
							<h5>Meta</h5>
							<p><span class="code">dictionary</span> returns all the entries in the process dictionary<sup><a href="#sub_5_19">19</a></sup> . Generally safe to use, because people shouldn’t be storing gigabytes of arbitrary data in there.</p>
							
							<p><span class="code">group_leader</span> the group leader of a process defines where IO (files, output of <span class="code">io:format/1-3</span>) goes.<sup><a href="#sub_5_20">20</a></sup> </p>
							
							<p><span class="code">registered_name</span> if the process has a name (as registered with <span class="code">erlang:register/2</span>), it is given here.</p>
							<p><span class="code">status</span> the nature of the process as seen by the scheduler. The possible values are:</p>
							<ul>
								<li><span class="code">exiting</span> the process is done, but not fully cleared yet;</li>
								<li><span class="code">waiting</span> the process is waiting in a <span class="code">receive ... end</span>;</li>
								<li><span class="code">running</span> self-descriptive;</li>
								<li><span class="code">runnable</span> ready to run, but not scheduled yet because another process is running;</li>
								<li><span class="code">garbage_collecting</span> self-descriptive;</li>
								<li><span class="code">suspended</span> whenever it is suspended by a BIF, or as a back-pressure mechanism because a socket or port buffer is full. The process only becomes runnable again once the port is no longer busy.
								</li>
							</ul>
							
							<h5>Signals</h5>
							
							<p><span class="code">links</span> will show a list of all the links a process has towards other processes and also ports (sockets, file descriptors). Generally safe to call, but to be used with care on large supervisors that may return thousands and thousands of entries. </p>
							
							<p><span class="code">monitored_by</span> gives a list of processes that are monitoring the current process (through the use of erlang:monitor/2).</p>
							
							<p><span class="code">monitors</span> kind of the opposite of monitored_by; it gives a list of all the processes being monitored by the one polled here.</p>
							<p><span class="code">trap_exit</span> has the value <span class="code">true</span> if the process is trapping exits, <span class="code">false</span> otherwise.</p>
							<sub>
							<p> <a id="sub_5_17">17</a> In cases where processes contain sensitive information, data can be forced to be kept private by calling <span class="code">process_flag(sensitive, true)</span></p>
							
							<p><a id="sub_5_18">18</a> For <span class="it">all</span> options, look at <a href="http://www.erlang.org/doc/man/erlang.html#process_info-2">http://www.erlang.org/doc/man/erlang.html#process_info-2</a></p>
							
							<p><a id="sub_5_19">19</a> See <a href="http://www.erlang.org/course/advanced.html#dict">http://www.erlang.org/course/advanced.html#dict</a> and <a href="http://ferd.ca/on-the-use-of-the-process-dictionary-in-erlang.html">http://ferd.ca/on-the-use-of-the-process-dictionary-in-erlang.html</a></p>
							
							<p><a id="sub_5_20">20</a> See <a href="http://learnyousomeerlang.com/building-otp-applications#the-application-behaviour">http://learnyousomeerlang.com/building-otp-applications#the-application-behaviour</a> and <a href="http://erlang.org/doc/apps/stdlib/io_protocol.html">http://erlang.org/doc/apps/stdlib/io_protocol.html</a> for more details.</p>
							</sub>
							<h5>Location</h5>
							
							<p><span class="code">current_function</span> displays the current running function, as a tuple of the form <span class="code">{Mod, Fun, Arity}</span>.</p>
							
							<p><span class="code">current_location</span> displays the current location within a module, as a tuple of the form <span class="code">{Mod, Fun, Arity, [{File, FileName}, {line, Num}]}</span>.</p>
							
							<p><span class="code">current_stacktrace</span> more verbose form of the preceding option; displays the current stacktrace as a list of ’current locations’.</p>
							
							<p><span class="code">initial_call</span> shows the function that the process was running when spawned, of the form <span class="code">{Mod, Fun, Arity}</span>. This may help identify what the process was spawned as, rather than what it’s running right now. </p>
							
							<h5>Memory Used</h5>
							
							<p><span class="code">binary</span> Displays the all the references to refc binaries<sup><a href="#sub_5_21">21</a></sup> along with their size. Can be unsafe to use if a process has a lot of them allocated.</p>
							
							<p><span class="code">garbage_collection</span> contains information regarding garbage collection in the process. The content is documented as ’subject to change’ and should be treated as such. The information tends to contains entries such as the number of garbage collections the process has went through, options for full-sweep garbage collections, and heap sizes.</p>
							
							<p><span class="code">heap_size</span> A typical Erlang process contains an ’old’ heap and a ’new’ heap, and goes through generational garbage collection. This entry shows the process’ heap size for the newest generation, and it usually includes the stack size. The value returned is in <span class="it">words</span>. </p>

							<p><span class="code">memory</span> Returns, in <span class="it">bytes</span>, the size of the process, including the call stack, the heaps, and internal structures used by the VM that are part of a process. </p>
							
							<p><span class="code">message_queue_len</span> Tells you how many messages are waiting in the mailbox of a process. </p>
							
							<p><span class="code">messages</span> Returns all of the messages in a process’ mailbox. This attribute is <span class="it">extremely</span> dangerous to request in production because mailboxes can hold millions of messages if you’re debugging a process that managed to get locked up. <span class="it">Always</span> call for the <span class="code">message_queue_len</span> first to make sure it’s safe to use.</p>
							
							<p><span class="code">total_heap_size</span> Similar to <span class="code">heap_size</span>, but also contains all other fragments of the heap, including the old one. The value returned is in <span class="it">words</span>.</p>
							
							<h5>Work</h5>
							
							<p><span class="code">reductions</span> The Erlang VM does scheduling based on <span class="it">reductions</span>, an arbitrary unit of work that allows rather portable implementations of scheduling (time-based scheduling is usually hard to make work efficiently on as many OSes as Erlang runs on). The higher the reductions, the more work, in terms of CPU and function calls, a process is doing.</p>
							
							<sub>
							<p><a id="sub_5_21">21</a> See <a href="#7_2_binaries">Section 7.2 Binaries</a></p>
							</sub>
							<p>Fortunately, for all the common ones that are also safe, recon contains the <span class="code">recon:info/1</span> function to help:</p>
<pre><code class="erlang">
1> recon:info("<0.12.0>").
[{meta,[{registered_name,rex},
{dictionary,[{’$ancestors’,[kernel_sup,<0.10.0>]},
{’$initial_call’,{rpc,init,1}}]},
{group_leader,<0.9.0>},
{status,waiting}]},
 {signals,[{links,[<0.11.0>]},
   {monitors,[]},
   {monitored_by,[]},
   {trap_exit,true}]},
 {location,[{initial_call,{proc_lib,init_p,5}},
    {current_stacktrace,[{gen_server,loop,6,
  [{file,"gen_server.erl"},{line,358}]},
{proc_lib,init_p_do_apply,3,
  [{file,"proc_lib.erl"},{line,239}]}]}]},
 {memory_used,[{memory,2808},
       {message_queue_len,0},
       {heap_size,233},
       {total_heap_size,233},
       {garbage_collection,[{min_bin_vheap_size,46422},
{min_heap_size,233},
{fullsweep_after,65535},
{minor_gcs,0}]}]},
 {work,[{reductions,35}]}]
</code></pre>
							<p>For the sake of convenience, <span class="code">recon:info/1</span> will accept any pid-like first argument and handle it: literal pids, strings ("<0.12.0>"), registered atoms, global names (<span class="code">{global, Atom}</span>), names registered with a third-party registry (e.g. with <span class="code">gproc: {via, gproc, Name}</span>), or tuples (<span class="code">{0,12,0}</span>). The process just needs to be local to the node you’re debugging.</p>
							
							<p>If only a category of information is wanted, the category can be used directly:</p>
<pre>
<code class="erlang">2> recon:info(self(), work).
{work,[{reductions,11035}]}
</code></pre>
							<p>or can be used in exactly the same way as <span class="code">process_info/2</span>:</p>
<pre>
<code class="erlang">3> recon:info(self(), [memory, status]).
[{memory,10600},{status,running}]
</code></pre>
							<p>This latter form can be used to fetch unsafe information.</p>
							
							<p>With all this data, it’s possible to find out all we need to debug a system. The challenge then is often to figure out, between this per-process data, and the global one, which process(es) should be targeted. </p>
							
							<p>When looking for high memory usage, for example it’s interesting to be able to list all of a node’s processes and find the top N consumers. Using the attributes above and the <span class="code">recon:proc_count(Attribute, N)</span> function, we can get these results:</p>
<pre>
<code class="erlang">4> recon:proc_count(memory, 3).
[{<0.26.0>,831448,
  [{current_function,{group,server_loop,3}},
   {initial_call,{group,server,3}}]},
 {<0.25.0>,372440,
  [user,
   {current_function,{group,server_loop,3}},
   {initial_call,{group,server,3}}]},
{<0.20.0>,372312,
  [code_server,
   {current_function,{code_server,loop,1}},
   {initial_call,{erlang,apply,2}}]}]
</code></pre>
							<p>Any of the attributes mentioned earlier can work, and for nodes with long-lived processes that can cause problems, it’s a fairly useful function.</p>
							
							<p>There is however a problem when most processes are short-lived, usually too short to inspect through other tools, or when a moving window is what we need (for example, what processes are busy accumulating memory or running code <span class="it">right now )</span>.</p>
							
							<p>For this use case, Recon has the <span class="code">recon:proc_window(Attribute, Num, Milliseconds)</span> function.</p>
							
							<p>It is important to see this function as a snapshot over a sliding window. A program’s timeline during sampling might look like this:</p>
<pre>
<code class="erlang">--w---- [Sample1] ---x-------------y----- [Sample2] ---z--->
</code></pre>
							
							<p>The function will take two samples at an interval defined by <span class="code">Milliseconds</span>.</p>
							
							<p>Some processes will live between <span class="code">w </span>and die at <span class="code">x</span>, some between <span class="code">y</span> and <span class="code">z</span>, and some between x and y. These samples will not be too significant as they’re incomplete. </p>
							
							<p>If the majority of your processes run between a time interval <span class="code">x</span> to <span class="code">y</span> (in absolute terms), you should make sure that your sampling time is smaller than this so that for many processes, their lifetime spans the equivalent of <span class="code">w</span> and <span class="code">z</span>. Not doing this can skew the results: long-lived processes that have 10 times the time to accumulate data (say reductions) will look like huge consumers when they’re not one.<sup><a href="#sub_5_22">22</a></sup></p>
							
							<p>The function, once running gives results like follows:</p>
<pre><code class="erlang">
5> recon:proc_window(reductions, 3, 500).
[{<0.46.0>,51728,
  [{current_function,{queue,in,2}},
   {initial_call,{erlang,apply,2}}]},
 {<0.49.0>,5728,
  [{current_function,{dict,new,0}},
   {initial_call,{erlang,apply,2}}]},
 {<0.43.0>,650,
  [{current_function,{timer,sleep,1}},
   {initial_call,{erlang,apply,2}}]}]
</code></pre>
							<p>With these two functions, it becomes possible to hone in on a specific process that is causing issues or misbehaving. </p>
							
							<a id="5_2_2_otp_processes"><h4>5.2.2 OTP Processes </h4></a>
							
							<p>When processes in question are OTP processes (most of the processes in a production system should definitely be OTP processes), you instantly win more tools to inspect them.</p>

							<p>In general the <span class="code">sys</span> module<sup><a href="#sub_5_23">23</a> is what you want to look into. Read the documentation on it and you’ll discover why it’s so useful. It contains the following features for any OTP process: </p>

							<ul>
								<li> logging of all messages and state transitions, both to the shell or to a file, or even in an internal buffer to be queried;</li>
								<li>statistics (reductions, message counts, time, and so on);</li>
								<li> fetching the status of a process (metadata including the state);</li>
								<li> fetching the state of a process (as in the <span class="code">#state{}</span> record);</li>
								<li>replacing that state</li>
								<li>custom debugging functions to be used as callbacks</li>
							</ul>
							<p>It also provides functionality to suspend or resume process execution.</p>
							<p>I won’t go into a lot of details about these functions, but be aware that they exist.</p>
							
							<sub>
							<p><a id="sub_5_22">22</a> Warning: this function depends on data gathered at two snapshots, and then building a dictionary with entries to differentiate them. This can take a heavy toll on memory when you have many tens of thousands of processes, and a little bit of time.</p>
							
							<p><a id="sub_5_23">23</a> <a href="http://www.erlang.org/doc/man/sys.html">http://www.erlang.org/doc/man/sys.html</a></p>
							</sub>
							
							<a id="5_2_3_ports"><h4>5.2.3 Ports</h4></a>

							<p>Similarly to processes, Erlang ports allow a lot of introspection. The info can be accessed by calling <span class="code">erlang:port_info(Port, Key)</span>, and more info is available through the <span class="code">inet</span> module. Most of it has been regrouped by the <span class="code">recon:port_info/1-2</span> functions, which work using a somewhat similar interface to their process-related counterparts. </p>

							<h5>Meta</h5>
							
							<p><span class="code">id</span> internal index of a port. Of no particular use except to differentiate ports.</p>
							
							<p><span class="code">name</span> type of the port — with names such as <span class="code">"tcp_inet"</span>, <span class="code">"udp_inet"</span>, or <span class="code">"efile"</span>, for example.</p>
							
							<p><span class="code">os_pid</span> if the port is not an inet socket, but rather represents an external process or program, this value contains the os pid related to the said external program.</p>
							
							<h5>Signals</h5>
							
							<p><span class="code">connected</span> Each port has a controlling process in charge of it, and this process’ pid is the <span class="code">connected</span> one. </p>
							
							<p><span class="code">links</span> ports can be linked with processes, much like other processes can be. The list of linked processes is contained here. Unless the process has been owned by or manually linked to a lot of processes, this should be safe to use.</p>
							
							<p><span class="code">monitors</span> ports that represent external programs can have these programs end up monitoring Erlang processes. These processes are listed here.</p>
							
							<h5>IO</h5>
							
							<p><span class="code">input</span> the number of bytes read from the port.</p>
							
							<p><span class="code">output</span> the number of bytes written to the port.</p>
							
							<h5>Memory Used</h5>
							
							<p><span class="code">memory</span> this is the memory (in bytes) allocated by the runtime system for the port. This number tends to be small-ish and excludes space allocated by the port itself.</p>
							
							<p><span class="code">queue_size</span> Port programs have a specific queue, called the driver queue<sup><a href="#sub_5_24">24</a></sup>. This return the size of this queue, in bytes.</p>
							
							<h5>Type-Specific</h5>
							
							<p><span class="code">Inet Ports</span> Returns inet-specific data, including statistics<sup><a href="#sub_5_25">25</a></sup>, the local address and port number for the socket (<span class="code">sockname</span>), and the inet options used<sup><a href="#sub_5_26">26</a></sup></p>
							
							<sub>
							<a id="sub_5_24">24</a> The driver queue is available to queue output from the emulator to the driver (data from the driver to the emulator is queued by the emulator in normal Erlang message queues). This can be useful if the driver has to wait for slow devices etc, and wants to yield back to the emulator.
							<a id="sub_5_25">25</a> <a href="http://www.erlang.org/doc/man/inet.html#getstat-1">http://www.erlang.org/doc/man/inet.html#getstat-1</a>
							<a id="sub_5_26">26</a> <a href="http://www.erlang.org/doc/man/inet.html#setopts-2">http://www.erlang.org/doc/man/inet.html#setopts-2</a>
							</sub>

							<p><span class="code">Others</span> currently no other form than inet ports are supported in recon, and an empty list is returned.</p>
							
							<p>The list can be obtained as follows:</p>
<pre><code class="erlang">1> recon:port_info("#Port<0.818>").
[{meta,[{id,6544},{name,"tcp_inet"},{os_pid,undefined}]},
 {signals,[{connected,<0.56.0>},
           {links,[<0.56.0>]},
           {monitors,[]}]},
 {io,[{input,0},{output,0}]},
 {memory_used,[{memory,40},{queue_size,0}]},
 {type,[{statistics,[{recv_oct,0},
                     {recv_cnt,0},
                     {recv_max,0},
                     {recv_avg,0},
                     {recv_dvi,...},
                     {...}|...]},
 {peername,{{50,19,218,110},80}},
 {sockname,{{97,107,140,172},39337}},
 {options,[{active,true},
           {broadcast,false},
           {buffer,1460},
           {delay_send,...},
           {...}|...]}]}]
</code></pre>
							<p>On top of this, functions to find out specific problematic ports exist the way they do for processes. The gotcha is that so far, recon only supports them for inet ports and with restricted attributes: the number of octets (bytes) sent, received, or both (<span class="code">send_oct</span>, <span class="code">recv_oct</span>, <span class="code">oct</span>, respectively), or the number of packets sent, received, or both (<span class="code">send_cnt</span>, <span class="code">recv_cnt</span>, <span class="code">cnt</span>, respectively).</p>
							
							<p>So for the cumulative total, which can help find out who is slowly but surely eating up all your bandwidth:</p>
<pre>
<code class="erlang">2> recon:inet_count(oct, 3).
[{#Port<0.6821166>,15828716661,
  [{recv_oct,15828716661},{send_oct,0}]},
 {#Port<0.6757848>,15762095249,
  [{recv_oct,15762095249},{send_oct,0}]},
 {#Port<0.6718690>,15630954707,
  [{recv_oct,15630954707},{send_oct,0}]}]
</code></pre>
							
							<p>Which suggest some ports are doing only input and eating lots of bytes. You can then use <span class="code">recon:port_info("#Port<0.6821166>") </span>to dig in and find who owns that socket, and what is going on with it.</p>
							
							<p>Or in any other case, we can look at what is sending the most data within any time window<sup><a href="#sub_5_27">27</a></sup> with the <span class="code">recon:inet_window(Attribute, Count, Milliseconds)</span> function:</p>
<pre>
<code class="erlang">3> recon:inet_window(send_oct, 3, 5000).
[{#Port<0.11976746>,2986216,[{send_oct,4421857688}]},
 {#Port<0.11704865>,1881957,[{send_oct,1476456967}]},
 {#Port<0.12518151>,1214051,[{send_oct,600070031}]}]
</code></pre>
							<p>For this one, the value in the middle of the tuple is what send_oct was worth (or any chosen attribute for each call) during the specific time interval chosen (5 seconds here).</p>
							
							<p>There is still some manual work involved into properly linking a misbehaving port to a process (and then possibly to a specific user or customer), but all the tools are in place.</p>
							<a id="5_3_exercises"><h3>Exercises</h3></a>
							<h4>Review Questions</h4>
							<ol>
								<li>What kind of values are reported for Erlang’s memory?</li>
								<li>What’s a valuable process-related metric for a global view?</li>
								<li>What’s a port, and how should it be monitored globally?</li>
								<li>Why can’t you trust <span class="code">top</span> or <span class="code">htop</span> for CPU usage with Erlang systems? What’s the
								alternative?</li>
								<li>Name two types of signal-related information available for processes</li>
								<li>How can you find what code a specific process is running?</li>
								<li>What are the different kinds of memory information available for a specific process?</li>
								<li>How can you know if a process is doing a lot of work?</li>
								<li>Name a few of the values that are dangerous to fetch when inspecting processes in a
								production system.</li>
								<li>What are some features provided to OTP processes through the sys module?</li>
								<li>What kind of values are available when inspecting inet ports?</li>
								<li>How can you find the type of a port (Files, TCP, UDP)?</li>
							</ol>
							<sub>
							<p>
								<a id="sub_5_27">27</a>See the explanations for the<span class="code">recon:proc_window/3</span> in the preceding subsection
								</p>
							</sub>

							<h4>Open-ended Questions</h4>
							<ol>
								<li>Why do you want a long time window available on global metrics?</li>
								<li>Which would be more appropriate between <span class="code">recon:proc_count/2</span> and <span class="code">recon:proc_window/3</span> to find issues with:
									<ol>
										<li>Reductions</li>
										<li>Memory</li>
										<li>Message queue length</li>
									</ol>
								</li>
								<li>How can you find information about who is the supervisor of a given process?</li>
								<li>When should you use <span class="code">recon:inet_count/2</span>? <span class="code">recon:inet_window/3</span>?</li>
								<li>What could explain the difference in memory reported by the operating system and the memory functions in Erlang?</li>
								<li>Why is it that Erlang can sometimes look very busy even when it isn’t?</li>
								<li>How can you find what proportion of processes on a node are ready to run, but can’t be scheduled right away?</li>
							</ol>
							<h4>Hands-On</h4>
							<p>Using the code at <a href="https://github.com/ferd/recon_demo">https://github.com/ferd/recon_demo</a>:</p>
							<ol>
								<li>What’s the system memory?</li>
								<li>Is the node using a lot of CPU resources?</li>
								<li>Is any process mailbox overflowing?</li>
								<li>Which chatty process (<span class="code">council_member</span>) takes the most memory?</li>
								<li>Which chatty process is eating the most CPU?</li>
								<li>Which chatty process is consuming the most bandwidth?</li>
								<li>Which chatty process sends the most messages over TCP? The least?</li>
								<li>Can you find out if a specific process tends to hold multiple connections or file descriptors open at the same time on a node?</li>
								<li>Can you find out which function is being called by the most processes at once on the node right now?</li>
							</ol>
							
							<a id="chapter_6"><h2>Chapter 6</h2></a>
							<h2>Reading Crash Dumps</h2>
							
							<p>Whenever an Erlang node crashes, it will generate a crash dump<sup><a href="#sub_6_1">1</a></sup>.</p>
							
							<p>The format is mostly documented in Erlang’s official documentation<sup><a href="#sub_6_2">2</a></sup>, and anyone willing to dig deeper inside of it will likely be able to figure out what data means by looking at that documentation. There will be specific data that is hard to understand without also understanding the part of the VM they refer to, but that might be too complex for this document</p>
							
							<p>The crash dump is going to be named <span class="code">erl_crash.dump</span> and be located wherever the Erlang process was running by default. This behaviour (and the file name) can be overridden by specifying the <span class="code">ERL_CRASH_DUMP</span> environment variable <sup><a href="#sub_6_3">3</a></sup>.</p>
							
							<a id="6_1_general_view"><h3>6.1 General View</h3></a>
							
							<p>Reading the crash dump will be useful to figure out possible reasons for a node to die <span class="it">a posteriori</span>. One way to get a quick look at things is to use recon’s <span class="code">erl_crashdump_analyzer.sh</span><sup><a href="#sub_6_4">4</a></sup> and run it on a crash dump:</p>

<pre><code class="erlang">$ ./recon/script/erl_crashdump_analyzer.sh erl_crash.dump
analyzing erl_crash.dump, generated on: Thu Apr 17 18:34:53 2014
Slogan: eheap_alloc: Cannot allocate 2733560184 bytes of memory
(of type "old_heap").
Memory:
===
processes: 2912 Mb
processes_used: 2912 Mb
system: 8167 Mb
atom: 0 Mb
atom_used: 0 Mb
binary: 3243 Mb
code: 11 Mb
ets: 4755 Mb
---
total: 11079 Mb
Different message queue lengths (5 largest different):
===
1 5010932
2 159
5 158
49 157
4 156
Error logger queue length:
===
0
File descriptors open:
===
UDP: 0
TCP: 19951
Files: 2
---
Total: 19953
Number of processes:
===
36496
Processes Heap+Stack memory sizes (words) used in the VM (5 largest
different):
===
1 284745853
1 5157867
1 4298223
2 196650
12 121536
Processes OldHeap memory sizes (words) used in the VM (5 largest
different):
===
 3 318187
 9 196650
14 121536
64 75113
15 46422
Process States when crashing (sum):
===
     1 Garbing
    74 Scheduled
 36421 Waiting
</code></pre>
							<sub>
								<p><a id="sub_6_1">1</a> If it isn’t killed by the OS for violating ulimits while dumping or didn’t segfault.</p>
								<p><a id="sub_6_2">2</a> <a href="http://www.erlang.org/doc/apps/erts/crash_dump.html">http://www.erlang.org/doc/apps/erts/crash_dump.html</a></p>
								<p><a id="sub_6_3">3</a> Heroku’s Routing and Telemetry teams use the heroku_crashdumps app to set the path and name of the crash dumps. It can be added to a project to name the dumps by boot time and put them in a pre-set location</p>
								<p><a id="sub_6_4">4</a> <a href="https://github.com/ferd/recon/blob/master/script/erl_crashdump_analyzer.sh">https://github.com/ferd/recon/blob/master/script/erl_crashdump_analyzer.sh</a></p>
							</sub>
							<p>This data dump won’t point out a problem directly to your face, but will be a good clue as to where to look. For example, the node here ran out of memory and had 11079 Mb out of 15 Gb used (I know this because that’s the max instance size we were using!) This can be a symptom of:</p>
							
							<ul>
								<li>memory fragmentation;</li>
								<li>memory leaks in C code or drivers;</li>
								<li>lots of memory that got to be garbage-collected before generating the crash dump <sup><a href="#sub_6_5">5</a></sup> .</li>
							</ul>
							
							<p>More generally, look for anything surprising for memory there. Correlate it with the number of processes and the size of mailboxes. One may explain the other.</p>
							
							<p>In this particular dump, one process had 5 million messages in its mailbox. That’s telling. Either it doesn’t match on all it can get, or it is getting overloaded. There are also dozens of processes with hundreds of messages queued up — this can point towards overload or contention. It’s hard to have general advice for your generic crash dump, but there still are a few pointers to help figure things out.</p>
							
							<sub> <p><a id="sub_6_5">5</a> Notably here is reference-counted binary memory, which sits in a global heap, but ends up being garbage-collected before generating the crash dump. The binary memory can therefore be underreported. See <a href="#chapter_7"> Chapter 7</a> for more details</p></sub>
							
							<a id="6_2_full_mailboxes"><h3>6.2 Full Mailboxes</h3></a>
							
							<p>For loaded mailboxes, looking at large counters is the best way to do it. If there is one large mailbox, go investigate the process in the crash dump. Figure out if it’s happening because it’s not matching on some message, or overload. If you have a similar node running, you can log on it and go inspect it. If you find out many mailboxes are loaded, you may want to use recon’s <span class="code">queue_fun.awk</span> to figure out what function they’re running at the time of the crash:</p>

<pre><code class="erlang">1 $ awk -v threshold=10000 -f queue_fun.awk /path/to/erl_crash.dump
2 MESSAGE QUEUE LENGTH: CURRENT FUNCTION
3 ======================================
4 10641: io:wait_io_mon_reply/2
5 12646: io:wait_io_mon_reply/2
6 32991: io:wait_io_mon_reply/2
7 2183837: io:wait_io_mon_reply/2
8 730790: io:wait_io_mon_reply/2
9 80194: io:wait_io_mon_reply/2
10 ...
</code></pre>
							<p>This one will run over the crash dump and output all of the functions scheduled to run for processes with at least 10000 messages in their mailbox. In the case of this run, the script showed that the entire node was locking up waiting on IO for <span class="code">io:format/2</span> calls, for example.</p>

							<a id="6_3_too_many_processes"><h3>6.3 Too Many (or too few) Processes</h3></a>
							
							<p>The process count is mostly useful when you know your node’s usual average count <sup><a href="#sub_6_6">6</a></sup>, in order to figure if it’s abnormal or not.</p>
							
							<p>A count that is higher than normal may reveal a specific leak or overload, depending on applications.</p>
							
							<p>If the process count is extremely low compared to usual, see if the node terminated with a slogan like:</p>
							
<pre>
<code class="erlang">Kernel pid terminated (application_controller)
							({application_terminated, <AppName>, shutdown})</code></pre>
								<p>In such a case, the issue is that a specific application <span class="code">(&ltAppName&gt</span>) has reached its maximal restart frequency within its supervisors, and that prompted the node to shut down. Error logs that led to the cascading failure should be combed over to figure things out.</p>
								
								<sub>
									<p><a id="sub_6_6">6</a> See subsection <a href="#5_1_3_processes">5.1.3 Processes</a> for details</p>
								</sub>
									
									<a id="6_4_too_many_ports"><h3>6.4. Too Many Ports</h3></a>
									<p>Similarly to the process count, the port count is simple and mostly useful when you know your usual values<sup><a href="#sub_6_7">7</a></sup>.</p>
									
									<p>A high count may be the result of overload, Denial of Service attacks, or plain old resource leaks. Looking at the type of port leaked (TCP, UDP, or files) can also help reveal if there was contention on specific resources, or if the code using them is just wrong.</p>
									
									<sub>
										<p><a id="sub_6_7">7</a> See subsection <a href="#5_1_4_ports">5.1.4 Ports</a> for details</p>
									</sub>
									<a id="6_5_cant_allocate_memory"><h3>6.5 Can’t Allocate Memory</h3></a>
									
									<p>These are by far the most common types of crashes you are likely to see. There’s so much to cover, that <a href="#chapter_7">Chapter 7</a> is dedicated to understanding them and doing the required debugging on live systems.</p>
									
									<p>In any case, the crash dump will help figure out what the problem was after the fact. The process mailboxes and individual heaps are usually good indicators of issues. If you’re running out of memory without any mailbox being outrageously large, look at the processes heap and stack sizes as returned by the recon script.</p>
									
									<p>In case of large outliers at the top, you know some restricted set of processes may be eating up most of your node’s memory. In case they’re all more or less equal, see if the amount of memory reported sounds like a lot.</p>
									
									<p>If it looks more or less reasonable, head towards the "Memory" section of the dump and check if a type (ETS or Binary, for example) seems to be fairly large. They may point towards resource leaks you hadn’t expected.</p>
									
									<a id="6_6_exercises"><h3>6.6 Exercises</h3></a>
									<h4>Review Questions</h4>
									<ol>
										<li>How can you choose where a crash dump will be generated?</li>
										<li>What are common avenues to explore if the crash dump shows that the node ran out of memory?</li>
										<li>What should you look for if the process count is suspiciously low?</li>
										<li>If you find the node died with a process having a lot of memory, what could you do to find out which one it was?</li>
									</ol>
									
									
									
									<h4>Hands-On</h4>
									<p>Using the analysis of a crash dump in Section 6.1:</p>
									<ol>
										<li>What are specific outliers that could point to an issue?</li>
										<li>Does it look like repeated errors are the issue? If not, what could it be?</li>
									</ol>
<a id="chapter_7"><h2>Chapter 7</h2></a>
<h2>Memory Leaks</h2>

<p>There are truckloads of ways for an Erlang node to bleed memory. They go from extremely simple to astonishingly hard to figure out (fortunately, the latter type is also rarer), and it’s possible you’ll never encounter any problem with them.</p>

<p>You will find out about memory leaks in two ways:</p>
	<ol>
		<li>A crash dump (see Chapter 6);</li>
		<li>By finding a worrisome trend in the data you are monitoring.</li>
	</ol>

<p>This chapter will mostly focus on the latter kind of leak, because they’re easier to investigate and see grow in real time. We will focus on finding what is growing on the node and common remediation options, handling binary leaks (they’re a special case), and detecting memory fragmentation.</p>
	
	<a id="7_1_common_sources_of_leaks"><h3>7.1 Common Sources of Leaks</h3></a>
		<p>Whenever someone calls for help saying "oh no, my nodes are crashing", the first step is always to ask for data. Interesting questions to ask and pieces of data to consider are:</p>
									<ul>
										<li>Do you have a crash dump and is it complaining about memory specifically? If not, the issue may be unrelated. If so, go dig into it, it’s full of data.</li>
										<li>Are the crashes cyclical? How predictable are they? What else tends to happen at around the same time and could it be related?</li>
										<li>Do crashes coincide with peaks in load on your systems, or do they seem to happen at more or less any time? Crashes that happen especially <span class="it">during</span> peak times are often due to bad overload management (see Chapter 3). Crashes that happen at any time, even when load goes down following a peak are more likely to be actual memory issues.</li>
									</ul>
		<p>If all of this seems to point towards a memory leak, install one of the metrics libraries mentioned in <a href="#chapter_5">Chapter 5</a> and/or <span class="code">recon</span> and get ready to dive in.<sup><a href="#sub_7_1">1</a></sup></p>

		<p>The first thing to look at in any of these cases is trends. Check for all types of memory using <span class="code">erlang:memory()</span> or some variant of it you have in a library or metrics system. Check for the following points:</p>
									<ul>
										<li>Is any type of memory growing faster than others?</li>
										<li>Is there any type of memory that’s taking the majority of the space available?</li>
										<li>Is there any type of memory that never seems to go down, and always up (other than atoms)?</li>
									</ul>
		<p>Many options are available depending on the type of memory that’s growing.</p>
			
			<a id="7_1_1_atom"><h4>7.1.1 Atom</h4></a>
				
				<p><span class="it">Don’t use dynamic atoms!</span> Atoms go in a global table and are cached forever. Look for places where you call <span class="code">erlang:binary_to_term/1</span> and <span class="code">erlang:list_to_atom/1</span>, and consider switching to safer variants (<span class="code">erlang:binary_to_term(Bin, [safe]</span>) and <span class="code">erlang:list_to_existing_atom/1</span>).</p>
			
				<p>If you use the <span class="code">xmerl</span> library that ships with Erlang, consider open source alternatives<sup><a href="#sub_7_2">2</a></sup> or figuring the way to add your own SAX parser that can be safe<sup><a href="#sub_7_3">3</a></sup>.</p>
			
				<p>If you do none of this, consider what you do to interact with the node. One specific case that bit me in production was that some of our common tools used random names to connect to nodes remotely, or generated nodes with random names that connected to each other from a central server.<sup><a href="#sub_7_4">4</a></sup> Erlang node names are converted to atoms, so just having this was enough to slowly but surely exhaust space on atom tables. Make sure you generate them from a fixed set, or slowly enough that it won’t be a problem in the long run.</p>
			
			<a id="7_1_2_binary"><h4>7.1.2 Binary</h4></a>
				<p>See Section <a href="#7_2_binaries">7.2 Binaries</a></p>
			
			<a id="7_1_3_code"><h4>7.1.3. Code</h4></a>
				<p>The code on an Erlang node is loaded in memory in its own area, and sits there until it is garbage collected. Only two copies of a module can coexist at one time, so looking for very large modules should be easy-ish.</p>

				<sub>
					<p><a id="sub_7_1">1</a> See <a href="#chapter_4">Chapter 4</a> if you need help to connect to a running node </p>
					<p><a id="sub_7_2">2</a> I don’t dislike <span class="code">exml</span> or <span class="code">erlsom</span></p>
					<p><a id="sub_7_3">3</a> See Ulf Wiger at <a href="http://erlang.org/pipermail/erlang-questions/2013-July/074901.html">http://erlang.org/pipermail/erlang-questions/2013-July/074901.html</a></p>
					<p><a id="sub_7_4">4</a> This is a common approach to figuring out how to connect nodes together: have one or two central nodes with fixed names, and have every other one log to them. Connections will then propagate automatically.</p>
				</sub>
				
				<p>If none of them stand out, look for code compiled with HiPE<sup><a href="#sub_7_5">5</a></sup>. HiPE code, unlike regular BEAM code, is native code and cannot be garbage collected from the VM when new versions are loaded. Memory can accumulate, usually very slowly, if many or large modules are native-compiled and loaded at run time.</p>
				
				<p>Alternatively, you may look for weird modules you didn’t load yourself on the node and panic if someone got access to your system!</p>
			<a id="7_1_4_ets"><h4>7.1.4 ETS</h4></a>
				
				<p>ETS tables are never garbage collected, and will maintain their memory usage as long as records will be left undeleted in a table. Only removing records manually (or deleting the table) will reclaim memory.</p>
				
				<p>In the rare cases you’re actually leaking ETS data, call the undocumented <span class="code">ets:i()</span> function in the shell. It will print out information regarding number of entries (<span class="code">size</span>) and how much memory they take (<span class="code">mem</span>). Figure out if anything is bad.</p>
				
				<p>It’s entirely possible all the data there is legit, and you’re facing the difficult problem of needing to shard your data set and distribute it over many nodes. This is out of scope for this book, so best of luck to you. You can look into compression of your tables if you need to buy time, however.<sup><a href="#sub_7_6">6</a></sup></p>
			
			<a id="7_1_5_processes"><h4>7.1.5 Processes</h4></a>
				<h5>Links and Monitors</h5>
					
					<p>Is the global process count indicative of a leak? If so, you may need to investigate unlinked processes, or peek inside supervisors’ children lists to see what may be weird-looking.</p>
					
					<p>Finding unlinked (and unmonitored) processes is easy to do with a few basic commands:</p>

<pre><code class="erlang">1> [P || P <- processes(),
[{_,Ls},{_,Ms}] <- [process_info(P, [links,monitors])],
[]==Ls, []==Ms].</code></pre>

					<sub>
						<p><a id="sub_7_5">5</a> <a href="http://www.erlang.org/doc/man/HiPE_app.html">http://www.erlang.org/doc/man/HiPE_app.html</a></p>
						<p><a id="sub_7_6">6</a> See the <span class="code">compressed</span> option for <span class="code">ets:new/2</span></p>
					</sub>
					
					<p>This will return a list of processes with neither. For supervisors, just fetching <span class="code">supervisor:count_children(SupervisorPidOrName)</span> and seeing what looks normal can be a good pointer.</p>

				<h5>Memory Used</h5>
					
					<p>The per-process memory model is briefly described in Subsection 7.3.2, but generally speaking, you can find which individual processes use the most memory by looking for their <span class="code">memory</span> attribute. You can look things up either as absolute terms or as a sliding window.</p>
					
					<p>For memory leaks, unless you’re in a predictable fast increase, absolute values are usually those worth digging into first:</p>

<pre><code class="erlang">1> recon:proc_count(memory, 3).
[{<0.175.0>,325276504,
  [myapp_stats,
   {current_function,{gen_server,loop,6}},
   {initial_call,{proc_lib,init_p,5}}]},
 {<0.169.0>,73521608,
  [myapp_giant_sup,
   {current_function,{gen_server,loop,6}},
   {initial_call,{proc_lib,init_p,5}}]},
 {<0.72.0>,4193496,
  [gproc,
   {current_function,{gen_server,loop,6}},
{initial_call,{proc_lib,init_p,5}}]}]</code></pre>
					
					<p>Attributes that may be interesting to check other than memory may be any other fields in Subsection 5.2.1, including <span class="code">message_queue_len</span>, but <span class="code">memory</span> will usually encompass all other types.</p>

				<h5>Garbage Collections</h5>
					
					<p>It is very well possible that a process uses lots of memory, but only for short periods of time. For long-lived nodes with a large overhead for operations, this is usually not a problem, but whenever memory starts being scarce, such spiky behaviour might be something you want to get rid of.</p>
									
					<p>Monitoring all garbage collections in real-time from the shell would be costly. Instead, setting up Erlang’s system monitor<sup><a href="#sub_7_7">7</a></sup> might be the best way to go at it.</p>
									
					<p>Erlang’s system monitor will allow you to track information such as long garbage collection periods and large process heaps, among other things. A monitor can temporarily be set up as follows:</p>
									
					<sub>
						<p><a id="sub_7_7">7</a> <a href="http://www.erlang.org/doc/man/erlang.html#system_monitor-2">http://www.erlang.org/doc/man/erlang.html#system_monitor-2</a></p>
					</sub>
<pre><code class="erlang">1> erlang:system_monitor().
undefined
2> erlang:system_monitor(self(), [{long_gc, 500}]).
undefined
3> flush().
Shell got {monitor,<4683.31798.0>,long_gc,
                   [{timeout,515},
                    {old_heap_block_size,0},
                    {heap_block_size,75113},
                    {mbuf_size,0},
                    {stack_size,19},
                    {old_heap_size,0},
                    {heap_size,33878}]}
5> erlang:system_monitor(undefined).
{<0.26706.4961>,[{long_gc,500}]}
6> erlang:system_monitor().
undefined</code></pre>
					
					<p>The first command checks that nothing (or nobody else) is using a system monitor yet — you don’t want to take this away from an existing application or coworker.</p>
					
					<p>The second command will be notified every time a garbage collection takes over 500 milliseconds. The result is flushed in the third command. Feel free to also check for <span class="code">{large_heap, NumWords}</span> if you want to monitor such sizes. Be careful to start with large values at first if you’re unsure. You don’t want to flood your process’ mailbox with a bunch of heaps that are 1-word large or more, for example.</p>
					
					<p>Command 5 unsets the system monitor (exiting or killing the monitor process also frees it up), and command 6 validates that everything worked.</p>
					
					<p>You can then find out if such monitoring messages tend to coincide with the memory increases that seem to result in leaks or overuses, and try to catch culprits before things are too bad. Quickly reacting and digging into the process (possibly with <span class="code">recon:info/1</span>) may help find out what’s wrong with the application.</p>

		<a id="7_1_6_nothing_in_particular"><h4>Nothing in Particular</h4></a>
			<p>If nothing seems to stand out in the preceding material, binary leaks (Section 7.2) and memory fragmentation (Section 7.3) may be the culprits. If nothing there fits either, it’s possible a C driver, NIF, or even the VM itself is leaking. Of course, a possible scenario is that load on the node and memory usage were proportional, and nothing specifically ended up being leaky or modified. The system just needs more resources or nodes.</p>
									
	<a id="7_2_binaries"><h3>7.2 Binaries</h3></a>
		<p>Erlang’s binaries are of two main types: ProcBins and Refc binaries<sup><a href="#sub_7_8">8</a></sup>. Binaries up to 64 bytes are allocated directly on the process’s heap, and their entire life cycle is spent in there. Binaries bigger than that get allocated in a global heap for binaries only, and each process to use one holds a local reference to it in its local heap. These binaries are referencecounted, and the deallocation will occur only once all references are garbage-collected from all processes that pointed to a specific binary.</p>
									
		<p>In 99% of the cases, this mechanism works entirely fine. In some cases, however, the process will either:</p>
		<ol>
			<li>do too little work to warrant allocations and garbage collection;</li>
			<li>eventually grow a large stack or heap with various data structures, collect them, then get to work with a lot of refc binaries. Filling the heap again with binaries (even though a virtual heap is used to account for the refc binaries’ real size) may take a lot of time, giving long delays between garbage collections.</li>
		</ol>
		
		<a id="7_2_1_detecting_leaks"><h4>7.2.1 Detecting Leaks</h4>
										
			<p>Detecting leaks for reference-counted binaries is easy enough: take a measure of all of each process’ list of binary references (using the <span class="code">binary</span> attribute), force a global garbage collection, take another snapshot, and calculate the difference.</p>
										
			<p>This can be done directly with <span class="code">recon:bin_leak(Max)</span> and looking at the node’s total memory before and after the call:</p>
<pre><code class="erlang">1> recon:bin_leak(5).
[{<0.4612.0>,-5580,
  [{current_function,{gen_fsm,loop,7}},
   {initial_call,{proc_lib,init_p,5}}]},
 {<0.17479.0>,-3724,
  [{current_function,{gen_fsm,loop,7}},
   {initial_call,{proc_lib,init_p,5}}]},
 {<0.31798.0>,-3648,
  [{current_function,{gen_fsm,loop,7}},
   {initial_call,{proc_lib,init_p,5}}]},
 {<0.31797.0>,-3266,
  [{current_function,{gen,do_call,4}},
   {initial_call,{proc_lib,init_p,5}}]},
 {<0.22711.1>,-2532,
  [{current_function,{gen_fsm,loop,7}},
   {initial_call,{proc_lib,init_p,5}}]}]
</code></pre>
			
			<sub>
				<p><a id="sub_7_8">8</a> <a href="http://www.erlang.org/doc/efficiency_guide/binaryhandling.html#id65798">http://www.erlang.org/doc/efficiency_guide/binaryhandling.html#id65798</a></p>
			</sub>
			
			<p>This will show how many individual binaries were held and then freed by each process as a delta. The value <span class="code">-5580</span> means there were 5580 fewer refc binaries after the call than before.</p>
			
			<p>It is normal to have a given amount of them stored at any point in time, and not all numbers are a sign that something is bad. If you see the memory used by the VM go down drastically after running this call, you may have had a lot of idling refc binaries.</p>
			
			<p>Similarly, if you instead see some processes hold impressively large numbers of them<sup><a href="#sub_7_9">9</a></sup>, that might be a good sign you have a problem.</p>
			
			<p>You can further validate the top consumers in total binary memory by using the special <span class="code">binary_memory</span> attribute supported in <span class="code">recon</span>:</p>

<pre><code class="erlang">1> recon:proc_count(binary_memory, 3).
[{<0.169.0>,77301349,
  [app_sup,
   {current_function,{gen_server,loop,6}},
   {initial_call,{proc_lib,init_p,5}}]},
 {<0.21928.1>,9733935,
  [{current_function,{erlang,hibernate,3}},
   {initial_call,{proc_lib,init_p,5}}]},
 {<0.12386.1172>,7208179,
  [{current_function,{erlang,hibernate,3}},
   {initial_call,{proc_lib,init_p,5}}]}]
</code></pre>
			
			<p>This will return the N top processes sorted by the amount of memory the refc binaries reference to hold, and can help point to specific processes that hold a few large binaries, instead of their raw amount. You may want to try running this function <span class="it">before</span> <span class="code">recon:bin_leak/1</span>, given the latter garbage collects the entire node first.</p>

		<a id="7_2_2_fixing_leaks"><h4>7.2.2 Fixing Leaks</h4></a>
			
			<p>Once you’ve established you’ve got a binary memory leak using <span class="code">recon:bin_leak(Max)</span>, it should be simple enough to look at the top processes and see what they are and what kindof work they do.</p>
			
			<p>Generally, refc binaries memory leaks can be solved in a few different ways, dependingon the source:</p>
			<ul>
				<li>call garbage collection manually at given intervals (icky, but somewhat efficient);</li>
				<li>stop using binaries (often not desirable);</li>
				<li>use <span class="code">binary:copy/1-2</span><sup><a href="#sub_7_10">10</a></sup> if keeping only a small fragment (usually less than 64 bytes) of a larger binary;<sup><a href="#sub_7_11">11</a></sup></li>
				<li>move work that involves larger binaries to temporary one-off processes that will die when they’re done (a lesser form of manual GC!);</li>
				<li>or add hibernation calls when appropriate (possibly the cleanest solution for inactiveprocesses).</li>
			</ul>
			
			<sub>
				<p><a id="sub_7_9">9</a> We’ve seen some processes hold hundreds of thousands of them during leak investigations at Heroku!</p>
			</sub>
			
			<p>The first two options are frankly not agreeable and should not be attempted before all else failed. The last three options are usually the best ones to be used.</p>
			<h5>Routing Binaries</h5>
				<p>There’s a specific solution for a specific use case some Erlang users have reported. The problematic use case is usually having a middleman process routing binaries from one process to another one. That middleman process will therefore acquire a reference to every binary passing through it and risks being a common major source of refc binaries leaks.</p>
				<p>The solution to this pattern is to have the router process return the pid to route to and let the original caller move the binary around. This will make it so that only processes that do need to touch the binaries will do so.</p>
				<p>A fix for this can be implemented transparently in the router’s API functions, withoutany visible change required by the callers.</p>
		
		<a id="7_3_memory_fragmentation"><h3>7.3 Memory Fragmentation</h3></a>
			
			<p>Memory fragmentation issues are intimately related to Erlang’s memory model, as described in <a href="#7_3_2_erlang_memory_model">Section 7.3.2 Erlang's memory model</a>. It is by far one of the trickiest issues of running long-lived Erlang nodes(often when individual node uptime reaches many months), and will show up relatively rarely.</p>
			
			<p>The general symptoms of memory fragmentation are large amounts of memory beingallocated during peak load, and that memory not going away after the fact. Thedamning factor will be that the node will internally report much lower usage (through <span class="code">erlang:memory()</span>) than what is reported by the operating system.</p>

			<a id="7_3_1_finding_fragmentation"><h4>Finding Fragmentation</h4></a>
				<p>The <span class="code">recon_alloc</span> module was developed specifically to detect and help point towards the resolution of such issues.</p>
				
				<sub>
					<p><a id="sub_7_10">10</a> <a href="http://www.erlang.org/doc/man/binary.html#copy-1">http://www.erlang.org/doc/man/binary.html#copy-1</a></p>
					<p><a id="sub_7_11">11</a> It might be worth copying even a larger fragment of a refc binary. For example, copying 10 megabytes off a 2 gigabytes binary should be worth the short-term overhead if it allows the 2 gigabytes binary to be garbage-collected while keeping the smaller fragment longer.</p>
				</sub>
				
				<h5>Check Allocated Memory</h5>
				
					<p>Calling <span class="code">recon_alloc:memory/1</span> will report various memory metrics with more flexibility than <span class="code">erlang:memory/0</span>. Here are the possibly relevant arguments:</p>
				
					<ol>
						<li>call <span class="code">recon_alloc:memory(usage)</span>. This will return a value between 0 and 1 representinga percentage of memory that is being actively used by Erlang terms versus the memory that the Erlang VM has obtained from the OS for such purposes. If the usage is close to 100%, you likely do not have memory fragmentation issues. You’re just using a lot of it.</li>
						
						<li>check if <span class="code">recon_alloc:memory(allocated)</span> matches what the OS reports.<sup><a href="#sub_7_12">12</a></sup> It should match it fairly closely if the problem is really about fragmentation or a memory leak from Erlang terms.</li>
					</ol>
					
					<p>That should confirm if memory seems to be fragmented or not.</p>
				
				<h5>Find the Guilty Allocator</h5>
					<p>Call <span class="code">recon_alloc:memory(allocated_types)</span> to see which type of util allocator (see <a href="#7_3_2_erlang_memory_model">Section 7.3.2 Erlang's memory model</a>) is allocating the most memory. See if one looks like an obvious culprit when you compare the results with <span class="code">erlang:memory()</span>.</p>
					
					<p>Try <span class="code">recon_alloc:fragmentation(current)</span>. The resulting data dump will show different allocators on the node with various usage ratios.<sup><a href="#sub_7_13">13</a></sup></p>
					
					<p>If you see very low ratios, check if they differ when calling <span class="code"> recon_alloc:fragmentation(max)</span>, which should show what the usage patterns were like under your max memory load.</p>
					
					<p>If there is a big difference, you are likely having issues with memory fragmentation for a few specific allocator types following usage spikes.</p>
			<a id="7_3_2_erlang_memory_model"><h4>Erlang’s Memory Model</h4></a>
					
					<h5>The Global Level</h5>
						
						<p>To understand where memory goes, one must first understand the many allocators being used. Erlang’s memory model, for the entire virtual machine, is hierarchical. As shown in Figure 7.1, there are two main allocators, and a bunch of sub-allocators (numbered 1-9)</p>
						
						<sub>
							<p><a id="sub_7_12">12</a> You can call <span class="code">recon_alloc:set_unit(Type)</span> to set the values reported by <span class="code">recon_alloc</span> in bytes, kilobytes, megabytes, or gigabytes</p>
							<p><a id="sub_7_13">13</a> More information is available at <a href="http://ferd.github.io/recon/recon_alloc.html">http://ferd.github.io/recon/recon_alloc.html</a></p>
						</sub>
						
						<img src="images/memory_allocators.png">
						
						<p>Figure 7.1: Erlang’s Memory allocators and their hierarchy. Not shown is the special <span class="it">super carrier</span>, optionally allowing to pre-allocate (and limit) all memory available to the Erlang VM since R16B03.</p>
						
						<p>The sub-allocators are the specific allocators used directly by Erlang code and the VM for most data types:<sup><a href="#sub_7_14">14</a></sup></p>
						
						<ol>
							<li><span class="code">temp_alloc</span>: does temporary allocations for short use cases (such as data living within a single C function call).</li>
							<li><span class="code">eheap_alloc</span>: heap data, used for things such as the Erlang processes’ heaps</li>
							<li><span class="code">binary_alloc</span>: the allocator used for reference counted binaries (what their ’global heap’ is). Reference counted binaries stored in an ETS table remain in this allocator.</li>
							<li><span class="code">ets_alloc</span>: ETS tables store their data in an isolated part of memory that isn’t garbage collected, but allocated and deallocated as long as terms are being stored in tables</li>
							<li><span class="code">driver_alloc</span>: used to store driver data in particular, which doesn’t keep drivers that generate Erlang terms from using other allocators. The driver data allocated here contains locks/mutexes, options, Erlang ports, etc.</li>
							<li><span class="code">sl_alloc</span>: short-lived memory blocks will be stored there, and include items such as some of the VM’s scheduling information or small buffers used for some data types’ handling.</li>
							<li><span class="code">ll_alloc</span>: long-lived allocations will be in there. Examples include Erlang code itself and the atom table, which stay there.</li>
							<li><span class="code">fix_alloc</span>: allocator used for frequently used fixed-size blocks of memory. One example of data used there is the internal processes’ C struct, used internally by the VM.</li>
							<li><span class="code">std_alloc</span>: catch-all allocator for whatever didn’t fit the previous categories. The process registry for named process is there.</li>
						</ol>
						
						<img src="images/memory_allocators2.png">
						<p>Figure 7.2: Example memory allocated in a specific sub-allocator</p>
						
						<p>By default, there will be one instance of each allocator per scheduler (and you should have one scheduler per core), plus one instance to be used by linked-in drivers using async threads. This ends up giving you a structure a bit like in Figure 7.1, but split it in N parts at each leaf.</p>
						
						<p>Each of these sub-allocators will request memory from <span class="code">mseg_alloc</span> and <span class="code">sys_alloc</span> depending on the use case, and in two possible ways. The first way is to act as a multiblock carrier (<span class="code">mbcs</span>), which will fetch chunks of memory that will be used for many Erlang terms at once. For each <span class="code">mbc</span>, the VM will set aside a given amount of memory (about 8MB by default in our case, which can be configured by tweaking VM options), and each term allocated will be free to go look into the many multiblock carriers to find some decent space in which to reside.</p>

						<p>Whenever the item to be allocated is greater than the single block carrier threshold (<span class="code">sbct</span>)<sup><a href="#sub_7_15">15</a></sup>, the allocator switches this allocation into a single block carrier (<span class="code">sbcs</span>). A single block carrier will request memory directly from <span class="code">mseg_alloc</span> for the first <span class="code">mmsbc</span><sup><a href="#sub_7_12">16</a></sup> entries, and then switch over to <span class="code">sys_alloc</span> and store the term there until it’s deallocated.</p>

						<p>So looking at something such as the binary allocator, we may end up with something similar to Figure 7.2</p>

						<sub>
							<p><a id="sub_7_15">15</a> <a href="http://erlang.org/doc/man/erts_alloc.html#M_sbct">http://erlang.org/doc/man/erts_alloc.html#M_sbct</a></p>
							<p><a id="sub_7_16">16</a> <a href="http://erlang.org/doc/man/erts_alloc.html#M_mmsbc">http://erlang.org/doc/man/erts_alloc.html#M_mmsbc</a></p>
						</sub>
						
						<img src="images/memory_allocators3.png">
						<p>Example memory allocated in a specific sub-allocator</p>
						
						<p>Whenever a multiblock carrier (or the first <span class="code">mmsbc</span><sup><a href="#sub_7_17">17</a> single block carriers) can be reclaimed, <span class="code">mseg_alloc</span> will try to keep it in memory for a while so that the next allocation spike that hits your VM can use pre-allocated memory rather than needing to ask the system for more each time.</p>
						
						<p>You then need to know the different memory allocation strategies of the Erlang virtual machine:</p>
						
						<ol>
							<li>Best fit (<span class="code">bf</span>)</li>
							<li>Address order best fit (<span class="code">aobf</span>)</li>
							<li>Address order first fit (<span class="code">aoff</span>)</li>
							<li>Address order first fit carrier best fit (<span class="code">aoffcbf</span>)</li>
							<li>Address order first fit carrier address order best fit (<span class="code">aoffcaobf</span>)</li>
							<li>Good fit (<span class="code">gf</span>)</li>
							<li>A fit (<span class="code">af</span>)</li>
						</ol>
						
						<p>Each of these strategies can be configured individually for each <span class="code">alloc_util</span> allocator<sup><a href="#sub_7_18">18</a></sup></p>
						
						<p>For <span class="it">best fit</span> (<span class="code">bf</span>), the VM builds a balanced binary tree of all the free blocks’ sizes, and will try to find the smallest one that will accommodate the piece of data and allocate it there. In Figure 7.3, having a piece of data that requires three blocks would likely end in area 3.</p>
						
						<sub>
							<p><a id="sub_7_17">17</a> <a href="http://erlang.org/doc/man/erts_alloc.html#M_mmsbc">http://erlang.org/doc/man/erts_alloc.html#M_mmsbc</a></p>
							<p><a id="sub_7_18">18</a> <a href="http://erlang.org/doc/man/erts_alloc.html#M_as">http://erlang.org/doc/man/erts_alloc.html#M_as</a></p>
						</sub>
						
						<img src="images/memory_allocators4.png">
						<p>Example memory allocated in a specific sub-allocator</p>
						
						<p><span class="it">Address order best fit</span> (<span class="code">aobf</span>) will work similarly, but the tree instead is based on the addresses of the blocks. So the VM will look for the smallest block available that can accommodate the data, but if many of the same size exist, it will favor picking one that has a lower address. If I have a piece of data that requires three blocks, I’ll still likely end up in area 3, but if I need two blocks, this strategy will favor the first <span class="code">mbcs</span> in Figure 7.3 with area 1 (instead of area 5). This could make the VM have a tendency to favor the same carriers for many allocations.</p>
						
						<p><span class="it">Address order first fit</span> (<span class="code">aoff</span>) will favor the address order for its search, and as soon as a block fits, <span class="code">aoff</span> uses it. Where <span class="code">aobf</span> and <span class="code">bf</span> would both have picked area 3 to allocate fourblocks in Figure 7.3, this one will get area 2 as a first priority given its address is lowest.In Figure 7.4, if we were to allocate four blocks, we’d favor block 1 to block 3 because itsaddress is lower, whereas <span class="code">bf</span> would have picked either 3 or 4, and <span class="code">aobf</span> would have picked 3.</p>
						
						<p><span class="it">Address order first fit carrier best fit</span> (<span class="code">aoffcbf</span>) is a strategy that will first favor a carrier that can accommodate the size and then look for the best fit within that one. So if we were to allocate two blocks in Figure 7.4, <span class="code">bf</span> and <span class="code">aobf</span> would both favor block 5, <span class="code">aoff</span> would pick block 1. <span class="code">aoffcbf</span> would pick area 2, because the first <span class="code">mbcs</span> can accommodate it fine, and area 2 fits it better than area 1.</p>
						
						<p><span class="it">Address order first fit carrier address order best fit</span> (<span class="code">aoffcaobf</span>) will be similar to <span class="code">aoffcbf</span>, but if multiple areas within a carrier have the same size, it will favor the one with the smallest address between the two rather than leaving it unspecified.</p>
						
						<p><span class="it">Good fit</span> (<span class="code">gf</span>) is a different kind of allocator; it will try to work like best fit (<span class="code">bf</span>), but will only search for a limited amount of time. If it doesn’t find a perfect fit there and then,it will pick the best one encountered so far. The value is configurable through the <span class="code">mbsd</span><sup><a href="#sub_7_19">19</a></sup> VM argument.</p>
						
						<p><span class="it">A fit</span> (<span class="code">af</span>), finally, is an allocator behaviour for temporary data that looks for a single existing memory block, and if the data can fit, af uses it. If the data can’t fit, af allocates a new one.</p>
						
						<p>Each of these strategies can be applied individually to every kind of allocator, so that the heap allocator and the binary allocator do not necessarily share the same strategy.</p>
						
						<p>Finally, starting with Erlang version 17.0, each <span class="code">alloc_util</span> allocator on each scheduler has what is called a <span class="code">mbcs</span> <span class="it">pool</span>. The <span class="code">mbcs</span> pool is a feature used to fight against memory fragmentation on the VM. When an allocator gets to have one of its multiblock carriers become mostly empty,<sup><a href="#sub_7_20">20</a></sup> the carrier becomes <span class="it">abandoned</span>.</p>
						
						<p>This abandoned carrier will stop being used for new allocations, until new multiblock carriers start being required. When this happens, the carrier will be fetched from the <span class="code">mbcs</span> pool. This can be done across multiple <span class="code">alloc_util</span> allocators of the same type across schedulers. This allows the VM to cache mostly-empty carriers without forcing deallocation of their memory.<sup><a href="#sub_7_21">21</a></sup> It also enables the migration of carriers across schedulers when they contain little data, according to their needs.</p>
						
						<h5>The Process Level</h5>

							<p>On a smaller scale, for each Erlang process, the layout still is a bit different. It basically has this piece of memory that can be imagined as one box:</p>

<pre><code class="erlang">[                      ]</code></pre>

<p>On one end you have the heap, and on the other, you have the stack:</p>

<pre><code class="erlang">[heap |         | stack]</code></pre>

							<p>In practice there’s more data (you have an old heap and a new heap, for generational
							GC, and also a virtual binary heap, to account for the space of reference-counted binaries on a specific sub-allocator not used by the process — <span class="code">binary_alloc</span> vs. <span class="code">eheap_alloc</span>):</p>

<pre><code class="erlang">[heap     ||      stack]</code></pre>
						
						<sub>
							<p><a id="sub_7_19">19</a> <a href="http://www.erlang.org/doc/man/erts_alloc.html#M_mbsd">http://www.erlang.org/doc/man/erts_alloc.html#M_mbsd</a>
							</p>
							<p><a id="sub_7_20">20</a> The threshold is configurable through <a href="http://www.erlang.org/doc/man/erts_alloc.html#M_acul">http://www.erlang.org/doc/man/erts_alloc.html#M_acul</a></p>
							<p><a id="sub_7_21">21</a> In cases this consumes too much memory, the feature can be disabled with the options <span class="code">+MBacul 0</span>.</p>
						</sub>
						
						<p>The space is allocated more and more up until either the stack or the heap can’t fit in anymore. This triggers a minor GC. The minor GC moves the data that can be kept into the old heap. It then collects the rest, and may end up reallocating more space.</p>
						
						<p>After a given number of minor GCs and/or reallocations, a full-sweep GC is performed, which inspects both the new and old heaps, frees up more space, and so on. When a process dies, both the stack and heap are taken out at once. reference-counted binaries are decreased, and if the counter is at 0, they vanish.</p>
						
						<p>When that happens, over 80% of the time, the only thing that happens is that the memory is marked as available in the sub-allocator and can be taken back by new processes or other ones that may need to be resized. Only after having this memory unused — and the multiblock carrier unused also — is it returned to <span class="code">mseg_alloc</span> or <span class="code">sys_alloc</span>, which may or may not keep it for a while longer.</p>
			
			<a id="7_3_3_fixing_memory_fragmentation"><h4>Fixing Memory Fragmentation with a Different Allocation Strategy</h4></a>

				<p>Tweaking your VM’s options for memory allocation may help.</p>

				<p>You will likely need to have a good understanding of what your type of memory load and usage is, and be ready to do a lot of in-depth testing. The <span class="code">recon_alloc</span> module contains a few helper functions to provide guidance, and the module’s documentation<sup><a href="#sub_7_22">22</a></sup> should be read at this point.</p>
				
				<p>You will need to figure out what the average data size is, the frequency of allocation and deallocation, whether the data fits in <span class="code">mbcs</span> or <span class="code">sbcs</span>, and you will then need to try playing with a bunch of the options mentioned in <span class="code">recon_alloc</span>, try the different strategies, deploy them, and see if things improve or if they impact times negatively.</p>
				
				<p>This is a very long process for which there is no shortcut, and if issues happen only every few months per node, you may be in for the long haul.</p>

		<a id="7_4_exercises"><h3>Exercises</h3></a>
		<h4>Review Questions</h4>
			<ol>
				<li>Name some of the common sources of leaks in Erlang programs.</li>
				<li>What are the two main types of binaries in Erlang?</li>
				<li>What could be to blame if no specific data type seems to be the source of a leak?</li>
				<li>If you find the node died with a process having a lot of memory, what could you do to find out which one it was?</li>
				<li>How could code itself cause a leak?</li>
				<li>How can you find out if garbage collections are taking too long to run?</li>
			</ol>
		<sub>
			<p><a id="sub_7_22">22</a> <a href="http://ferd.github.io/recon/recon_alloc.html">http://ferd.github.io/recon/recon_alloc.html</a></p>
		</sub>
		<h4>Open-ended Questions</h4>
			<ol>
				<li>How could you verify if a leak is caused by forgetting to kill processes, or by processes using too much memory on their own?</li>
				<li>A process opens a 150MB log file in binary mode to go extract a piece of informationfrom it, and then stores that information in an ETS table. After figuring out you have a binary memory leak, what should be done to minimize binary memory usage on the node?</li>
				<li>What could you use to find out if ETS tables are growing too fast?</li>
				<li>What steps should you go through to find out that a node is likely suffering from fragmentation? How could you disprove the idea that is could be due to a NIF or driver leaking memory?</li>
				<li>How could you find out if a process with a large mailbox (from reading message_queue_len) seems to be leaking data from there, or never handling new messages?</li>
				<li>A process with a large memory footprint seems to be rarely running garbage collections. What could explain this?</li>
				<li>When should you alter the allocation strategies on your nodes? Should you prefer to tweak this, or the way you wrote code?</li>
			</ol>
		<h4>Hands-On</h4>
			<ol>
				<li>Using any system you know or have to maintain in Erlang (including toy systems),can you figure out if there are any binary memory leaks on there?</li>
		</ol>

<a id="chapter_8"><h2>Chapter 8</h2></a>
<h2>CPU and Scheduler Hogs</h2>

	<p>While memory leaks tend to absolutely kill your system, CPU exhaustion tends to act like
	a bottleneck and limits the maximal work you can get out of a node. Erlang developers
	will have a tendency to scale horizontally when they face such issues. It is often an easy
	enough job to scale out the more basic pieces of code out there. Only centralized global
	state (process registries, ETS tables, and so on) usually need to be modified.<sup><a href="#sub_8_1">1</a></sup> Still, if you
	want to optimize locally before scaling out at first, you need to be able to find your CPU
	and scheduler hogs.</p>

	<p>It is generally difficult to properly analyze the CPU usage of an Erlang node to pin
	problems to a specific piece of code. With everything concurrent and in a virtual machine,
	there is no guarantee you will find out if a specific process, driver, your own Erlang code,
	NIFs you may have installed, or some third-party library is eating up all your processing
	power.</p>

	<p>The existing approaches are often limited to profiling and reduction-counting if it’s in
	your code, and to monitoring the scheduler’s work if it might be anywhere else (but also
	your code).</p>

	<a id="8_1_profiling_and_reduction_counts"><h3>8.1 Profiling and Reduction Counts</h3></a>

		<p>To pin issues to specific pieces of Erlang code, as mentioned earlier, there are two main
		approaches. One will be to do the old standard profiling routine, likely using one of the
		following applications:<sup><a href="#sub_8_2">2</a></sup></p>
		<sub>
				<p><a id="sub_8_1">1</a> Usually this takes the form of sharding or finding a state-replication scheme that’s suitable, and little more. It’s still a decent piece of work, but nothing compared to finding out most of your program’s semantics aren’t applicable to distributed systems given Erlang usually forces your hand there in the first place.</p>
				<p><a id="sub_8_2">2</a> All of these profilers work using Erlang tracing functionality with almost no restraint. They will have an impact on the run-time performance of the application, and shouldn’t be used in production.</p>
			
		</sub>
		<ul>
			<li><span class="code">eprof</span>,<sup><a href="#sub_8_3">3</a></sup> the oldest Erlang profiler around. It will give general percentage values and will mostly report in terms of time taken.</li>
			<li><span class="code">fprof</span>,<sup><a href="#sub_8_4">4</a></sup> a more powerful replacement of eprof. It will support full concurrency and generate in-depth reports. In fact, the reports are so deep that they are usually considered opaque and hard to read.</li>
			<li><span class="code">eflame</span>,<sup><a href="#sub_8_5">5</a></sup> the newest kid on the block. It generates flame graphs to show deep call sequences and hot-spots in usage on a given piece of code. It allows one to quickly find issues with a single look at the final result.</li>
		</ul>
		
		<p>It will be left to the reader to thoroughly read each of these application’s documentation.
		The other approach will be to run <span class="code">recon:proc_window/3</span> as introduced in Subsection 5.2.1:</p>

<pre><code class="erlang">1> recon:proc_window(reductions, 3, 500).
[{<0.46.0>,51728,
  [{current_function,{queue,in,2}},
   {initial_call,{erlang,apply,2}}]},
 {<0.49.0>,5728,
  [{current_function,{dict,new,0}},
   {initial_call,{erlang,apply,2}}]},
 {<0.43.0>,650,
  [{current_function,{timer,sleep,1}},
{initial_call,{erlang,apply,2}}]}]</code></pre>

		<p>The reduction count has a direct link to function calls in Erlang, and a high count is usually the synonym of a high amount of CPU usage.</p>

		<p>What’s interesting with this function is to try it while a system is already rather busy,<sup><a href="#sub_8_6">6</a></sup> with a relatively short interval. Repeat it many times, and you should hopefully see a pattern emerge where the same processes (or the same <span class="it">kind</span> of processes) tend to always come up on top.</p>

		<p>Using the code locations<sup><a href="#sub_8_7">7</a></sup> and current functions being run, you should be able to identify what kind of code hogs all your schedulers.</p>
	
	<a id="8_2_system_monitors"><h3>8.2 System Monitors</h3></a>
		<p>If nothing seems to stand out through either profiling or checking reduction counts, it’s possible some of your work ends up being done by NIFs, garbage collections, and so on.</p>
		<sub>
			<p><a id="sub_8_3">3</a> <a href="http://www.erlang.org/doc/man/eprof.html">http://www.erlang.org/doc/man/eprof.html</a></p>
			<p><a id="sub_8_4">4</a> <a href="http://www.erlang.org/doc/man/fprof.html">http://www.erlang.org/doc/man/fprof.html</a></p>
			<p><a id="sub_8_5">5</a> <a href="https://github.com/proger/eflame">https://github.com/proger/eflame</a></p>
			<p><a id="sub_8_5">6 See <a href="#5_1_2_cpu">Subsection 5.1.2 CPU</a></p>
			<p><a id="sub_8_7">7</a> Call <span class="code">recon:info(PidTerm, location)</span> or <span class="code">process_info(Pid, current_stacktrace)</span> to get this information.</p>
			
		</sub>
		
		<p>These kinds of work may not always increment their reductions count correctly, so they won’t show up with the previous methods, only through long run times.</p>
		
		<p>To find about such cases, the best way around is to use <span class="code">erlang:system_monitor/2</span>, and look for <span class="code">long_gc</span> and <span class="code">long_schedule</span>. The former will show whenever garbage collection ends up doing a lot of work (it takes time!), and the latter will likely catch issues with busy processes, either through NIFs or some other means, that end up making them hard to de-schedule.<sup><a href="#sub_8_8">8</a></sup></p>
		
		<p>We’ve seen how to set such a system monitor In Garbage Collection in 7.1.5, but here’s a different pattern<sup><a href="#sub_8_9">9</a></sup> I’ve used before to catch long-running items:</p>
<pre><code class="erlang">1> F = fun(F) ->
receive
{monitor, Pid, long_schedule, Info} ->
io:format("monitor=long_schedule pid=~p info=~p~n", [Pid, Info]);
{monitor, Pid, long_gc, Info} ->
io:format("monitor=long_gc pid=~p info=~p~n", [Pid, Info])
end,
F(F)
end.
2> Setup = fun(Delay) -> fun() ->
register(temp_sys_monitor, self()),
erlang:system_monitor(self(), [{long_schedule, Delay}, {long_gc, Delay}]),
F(F)
end end.
3> spawn_link(Setup(1000)).
<0.1293.0>
monitor=long_schedule pid=<0.54.0> info=[{timeout,1102},
                                         {in,{some_module,some_function,3}},
                                         {out,{some_module,some_function,3}}]
</code></pre>
		
		<p>Be sure to set the <span class="code">long_schedule</span> and <span class="code">long_gc</span> values to large-ish values that might be reasonable to you. In this example, they’re set to 1000 milliseconds. You can either kill the monitor by calling <span class="code">exit(whereis(temp_sys_monitor), kill)</span> (which will in turn kill the shell because it’s linked), or just disconnect from the node (which will kill the process because it’s linked to the shell.)</p>
		
		<p>This kind of code and monitoring can be moved to its own module where it reports to a long-term logging storage, and can be used as a canary for performance degradation or overload detection.</p>
		
		<sub>
			<il><a id="sub_8_8">8</a> Long garbage collections count towards scheduling time. It is very possible that a lot of your long schedules will be tied to garbage collections depending on your system.</il>
			<il><a id="sub_8_9">9</a> If you’re on 17.0 or newer versions, the shell functions can be made recursive far more simply by using their named form, but to have the widest compatibility possible with older versions of Erlang, I’ve let them as is.</il>
		</sub>
		
		<a id="8_2_1_suspended_ports"><h4>8.2.1 Suspended Ports</h4></a>
		
			<p>An interesting part of system monitors that didn’t fit anywhere but may have to do with scheduling is regarding ports. When a process sends too many message to a port and the port’s internal queue gets full, the Erlang schedulers will forcibly de-schedule the sender until space is freed. This may end up surprising a few users who didn’t expect that implicit back-pressure from the VM.</p>
		
			<p>This kind of event can be monitored by passing in the atom <span class="code">busy_port</span> to the system monitor. Specifically for clustered nodes, the atom <span class="code">busy_dist_port</span> can be used to find when a local process gets de-scheduled when contacting a process on a remote node whose inter-node communication was handled by a busy port.</p>
		
			<p>If you find out you’re having problems with these, try replacing your sending functions where in critical paths with <span class="code">erlang:port_command(Port, Data, [nosuspend])</span> for ports, and <span class="code">erlang:send(Pid, Msg, [nosuspend])</span> for messages to distributed processes. They will then tell you when the message could not be sent and you would therefore have been descheduled.</p>

	<a id="8_3_exercises"><h3>8.3 Exercises</h3></a>
		<h4>Review Questions</h4>
			<ol>
				<il>What are the two main approaches to pin issues about CPU usages?</il>
				<il>Name some of the profiling tools available. What approaches are preferable for production use? Why?</il>
				<il>Why can long scheduling monitors be useful to find CPU or scheduler over-consumption?</il>
			</ol>
		<h4>Open-ended Questions</h4>
			<ol>
				<il>If you find that a process doing very little work with reductions ends up being scheduled for long periods of time, what can you guess about it or the code it runs?</il>
				<il>Can you set up a system monitor and then trigger it with regular Erlang code? Can you use it to find out for how long processes seem to be scheduled on average? You may need to manually start random processes from the shell that are more aggressive in their work than those provided by the existing system.</il>
			</ol>

<a id="chapter_9"><h2>Chapter 9</h2></a>

<h2>Tracing</h2>
	<p>One of the lesser known and absolutely under-used features of Erlang and the BEAM virtual machine is just about how much tracing you can do on there.</p>
	
	<p>Forget your debuggers, their use is too limited.<sup><a href="#sub_9_1">1</a></sup> Tracing makes sense in Erlang at all steps of your system’s life cycle, whether it’s for development or for diagnosing a running production system.</p>
	
	<p>There are a few options available to trace Erlang programs:</p>
		<ul>
			<li><span class="code">sys</span><sup><a href="#sub_9_2">2</a></sup> comes standard with OTP and allows the user to set custom tracing functions, log all kinds of events, and so on. It’s generally complete and fine to use for development. It suffers a bit in production because it doesn’t redirect IO to remote shells, and doesn’t have rate-limiting capabilities for trace messages. It is still recommended to read the documentation for the module.</li>
			
			<li><span class="code">dbg</span><sup><a href="#sub_9_3">3</a></sup> also comes standard with Erlang/OTP. Its interface is a bit clunky in terms of usability, but it’s entirely good enough to do what you need. The problem with it is that you <span class="it">have to know what you’re doing</span>, because <span class="code">dbg</span> can log absolutely everything on the node and kill one in under two seconds.</li>
			
			<li><span class="it">tracing BIFs</span> are available as part of the <span class="code">erlang</span> module. They’re mostly the raw blocks used by all the applications mentioned in this list, but their lower level of abstraction makes them rather difficult to use.</li>
			
			<li><span class="code">redbug</span><sup><a href="#sub_9_4">4</a></sup> is a production-safe tracing library, part of the <span class="code">eper</span><sup><a href="#sub_9_5">5</a></sup> suite. It has an internalrate-limiter, and a nice usable interface. To use it, you must however be willing to add in all of <span class="code">eper</span>’s dependencies. The toolkit is fairly comprehensive and can be a very interesting install.</li>
			
			<li><span class="code">recon_trace</span><sup><a href="#sub_9_6">6</a></sup> is <span class="code">recon</span>’s take on tracing. The objective was to allow the same levels of safety as with <span class="code">redbug</span>, but without the dependencies. The interface is different, and the rate-limiting options aren’t entirely identical. It can also only trace function calls, and not messages.<sup><a href="#sub_9_7">7</a></sup></li>
		</ul>
		
		<p>This chapter will focus on tracing with <span class="code">recon_trace</span>, but the terminology and the concepts used mostly carry over to any other Erlang tracing tool that can be used.</p>
		
		<sub>
			<p><a id="sub_9_1">1</a> Tne common issue with debuggers that let you insert break points and step through a program is that they are incompatible with many Erlang programs: put a break point in one process and the ones around keep going. In practice, this turns debugging into a very limited activity because as soon as a process needs to interact with the one you’re debugging, its calls start timing out and crashing, possibly taking down the entire node with it. Tracing, on the other hand, doesn’t interfere with program execution, but still gives you all the data you need.</p>
			<p><a id="sub_9_2">2</a> <a href="http://www.erlang.org/doc/man/sys.html">http://www.erlang.org/doc/man/sys.html</a></p>
			<p><a id="sub_9_3">3</a> <a href="http://www.erlang.org/doc/man/dbg.html">http://www.erlang.org/doc/man/dbg.html</a></p>
		</sub>
		
		<a id="9_1_tracing_principles"><h3>9.1 Tracing Principles</h3></a>
		
			<p>The Erlang Trace BIFs allow to trace any Erlang code at all<sup><a href="#sub_9_8">8</a></sup>. They work in two parts: <span class="it">pid specifications</span>, and <span class="it">trace patterns</span>.</p>
		
			<p>Pid specifications lets the user decide which processes to target. They can be specific pids, <span class="code">all</span> pids, <span class="code">existing</span> pids, or <span class="code">new</span> pids (those not spawned at the time of the function call).</p>
		
			<p>The trace patterns represent functions. Functions can be specified in two parts: specifying the modules, functions, and arity, and then with Erlang match specifications<sup><a href="#sub_9_9">9</a></sup> to add constraints to arguments.</p>
		
			<p>What defines whether a specific function call gets traced or not is the intersection of both, as seen in Figure 9.1.</p>
		
			<p>If either the pid specification excludes a process or a trace pattern excludes a given call, no trace will be received.</p>
		
			<p>Tools like <span class="code">dbg</span> (and trace BIFs) force you to work with this Venn diagram in mind. You specify sets of matching pids and sets of trace patterns independently, and whatever happens to be at the intersection of both sets gets to be displayed.</p>
		
			<p>Tools like <span class="code">redbug</span> and <span class="code">recon_trace</span>, on the other hand, abstract this away.</p>
			
			<sub>
				<p><a id="sub_9_4">4</a> <a href="https://github.com/massemanet/eper/blob/master/doc/redbug.txt">https://github.com/massemanet/eper/blob/master/doc/redbug.txt</a></p>
				<p><a id="sub_9_5">5</a> <a href="https://github.com/massemanet/eper">https://github.com/massemanet/eper</a></p>
				<p><a id="sub_9_6">6</a> <a href="http://ferd.github.io/recon/recon_trace.html">http://ferd.github.io/recon/recon_trace.html</a></p>
				<p><a id="sub_9_7">7</a> Messages may be supported in future iterations of the library. In practice, the author hasn’t found the need when using OTP, given behaviours and matching on specific arguments allows the user to get something roughly equivalent.</p>
				<p><a id="sub_9_8">8</a> In cases where processes contain sensitive information, data can be forced to be kept private by calling <span class="code">process_flag(sensitive, true)</span></p>
				<p><a id="sub_9_9">9</a> <a href="http://www.erlang.org/doc/apps/erts/match_spec.html">http://www.erlang.org/doc/apps/erts/match_spec.html</a></p>
			</sub>
			
			<img src="images/trace.png">
			<p>Figure 9.1: What gets traced is the result of the intersection between the matching pidsand the matching trace patterns</p>
			
			<a id="9_2_tracing_with_recon"><h3>9.2 Tracing with Recon</h3></a>
			
				<p>Recon, by default, will match all processes. This will often be good enough for a lotof debugging cases. The interesting part you’ll want to play with most of the time is specification of trace patterns. Recon support a few basic ways to declare them.</p>
			
				<p>The most basic form is <span class="code">{Mod, Fun, Arity}</span>, where Mod is a literal module, <span class="code">Fun</span> is a function name, and <span class="code">Arity</span> is the number of arguments of the function to trace. Any of these may also be replaced by wildcards (’_’). Recon will forbid forms that match too widely on everything (such as {’_’,’_’,’_’}), as they could be plain dangerous to run in production.</p>
			
				<p>A fancier form will be to replace the arity by a function to match on lists of arguments. The function is limited to those usable by match specifications similar to what is available in ETS<sup><a href="#sub_9_10">10</a></sup>. Finally, multiple patterns can be put into a list to broaden the matching scope.</p>
			
				<p>It will also be possible to rate limit based on two manners: a static count, or a number of matches per time interval.</p>
			
				<sub>
				<p><a id="sub_9_10">10</a>	<a href="http://www.erlang.org/doc/man/ets.html#fun2ms-1">http://www.erlang.org/doc/man/ets.html#fun2ms-1</a></p>
				</sub>
					<p>Rather than going more in details, here’s a list of examples and how to trace for them.</p>
<pre><code class="erlang">%% All calls from the queue module, with 10 calls printed at most:
recon_trace:calls({queue, ’_’, ’_’}, 10)
%% All calls to lists:seq(A,B), with 100 calls printed at most:
recon_trace:calls({lists, seq, 2}, 100)
%% All calls to lists:seq(A,B), with 100 calls per second at most:
recon_trace:calls({lists, seq, 2}, {100, 1000})
%% All calls to lists:seq(A,B,2) (all sequences increasing by two) with 100 calls
%% at most:
recon_trace:calls({lists, seq, fun([_,_,2]) -> ok end}, 100)
%% All calls to iolist_to_binary/1 made with a binary as an argument already
%% (a kind of tracking for useless conversions):
recon_trace:calls({erlang, iolist_to_binary,
                   fun([X]) when is_binary(X) -> ok end},
                   10)
%% Calls to the queue module only in a given process Pid, at a rate of 50 per
%% second at most:
recon_trace:calls({queue, ’_’, ’_’}, {50,1000}, [{pid, Pid}])
%% Print the traces with the function arity instead of literal arguments:
recon_trace:calls(TSpec, Max, [{args, arity}])
%% Matching the filter/2 functions of both dict and lists modules, across new
%% processes only:
recon_trace:calls([{dict,filter,2},{lists,filter,2}], 10, [{pid, new}])
%% Tracing the handle_call/3 functions of a given module for all new processes,
%% and those of an existing one registered with gproc:
recon_trace:calls({Mod,handle_call,3}, {1,100}, [{pid, [{via, gproc, Name}, new]}
%% Show the result of a given function call, the important bit being the
%% return_trace() call or the {return_trace} match spec value.
recon_trace:calls({Mod,Fun,fun(_) -> return_trace() end}, Max, Opts)
recon_trace:calls({Mod,Fun,[{’_’, [], [{return_trace}]}]}, Max, Opts)
</code></pre>
					<p>Each call made will override the previous one, and all calls can be cancelled with <span class="code">recon_trace:clear/0</span>.</p>
					<p>There’s a few more combination possible, with more options:</p>
					
					<p><span class="code">{pid, PidSpec}</span></p>
					
					<p>Which processes to trace. Valid options is any of <span class="code">all</span>, <span class="code">new</span>, <span class="code">existing</span>, or a process descriptor (<span class="code">{A,B,C}</span>, <span class="code">"<A.B.C>"</span>, an atom representing a name, <span class="code">{global, Name},{via, Registrar, Name}</span>, or a pid). It’s also possible to specify more than one by putting them in a list.</p>
					
					<p><span class="code">{timestamp, formatter | trace}</span></p>
					
					<p>By default, the formatter process adds timestamps to messages received. If accurate timestamps are required, it’s possible to force the usage of timestamps within trace messages by adding the option <span class="code">{timestamp, trace}</span>.</p>
					
					<p><span class="code">{args, arity | args}</span></p>
					
					<p>Whether to print the arity in function calls or their (by default) literal representation.</p>
					
					<p><span class="code">{scope, global | local}</span></p>
					
					<p>By default, only ’global’ (fully qualified function calls) are traced, not calls made internally. To force tracing of local calls, pass in <span class="code">{scope, local}</span>. This is useful whenever you want to track the changes of code in a process that isn’t called with <span class="code">Module:Fun(Args)</span>, but just <span class="code">Fun(Args)</span>.</p>
					
					<p>With these options, the multiple ways to pattern match on specific calls for specific functions and whatnot, a lot of development and production issues can more quickly be diagnosed. If the idea ever comes to say "hm, maybe I should add more logging there to see what could cause that funny behaviour", tracing can usually be a very fast shortcut to get the data you need without deploying any code or altering its readability.</p>
				
				<a id="9_3_example_sessions"><h3>9.3 Example Sessions</h3></a>
					<p>First let’s trace the queue:new functions in any process:</p>
<pre><code class="erlang">1> recon_trace:calls({queue, new, ’_’}, 1).
1
13:14:34.086078 <0.44.0> queue:new()
Recon tracer rate limit tripped.</code></pre>

					<p>The limit was set to 1 trace message at most, and recon let us know when that limit was reached.</p>

					<p>Let’s instead look for all the <span class="code">queue:in/2</span> calls, to see what it is we’re inserting in queues:</p>

<pre><code class="erlang">2> recon_trace:calls({queue, in, 2}, 1).
1
13:14:55.365157 <0.44.0> queue:in(a, {[],[]})
Recon tracer rate limit tripped.
</code></pre>
					<p>In order to see the content we want, we should change the trace patterns to use a fun that matches on all arguments in a list (_) and returns <span class="code">return_trace()</span>. This last part will generate a second trace for each call that includes the return value:</p>

<pre><code class="erlang">3> recon_trace:calls({queue, in, fun(_) -> return_trace() end}, 3).
1
13:15:27.655132 <0.44.0> queue:in(a, {[],[]})
13:15:27.655467 <0.44.0> queue:in/2 --> {[a],[]}
13:15:27.757921 <0.44.0> queue:in(a, {[],[]})
Recon tracer rate limit tripped.</code></pre>

					<p>Matching on argument lists can be done in a more complex manner:</p>

<pre><code class="erlang">4> recon_trace:calls(
4>   {queue, ’_’,
4>    fun([A,_]) when is_list(A); is_integer(A) andalso A > 1 ->
4>        return_trace()
4>    end},
4>   {10,100}
4> ).
32
13:24:21.324309 <0.38.0> queue:in(3, {[],[]})
13:24:21.371473 <0.38.0> queue:in/2 --> {[3],[]}
13:25:14.694865 <0.53.0> queue:split(4, {[10,9,8,7],[1,2,3,4,5,6]})
13:25:14.695194 <0.53.0> queue:split/2 --> {{[4,3,2],[1]},{[10,9,8,7],[5,6]}}
5> recon_trace:clear().
ok
</code></pre>
					
					<p>Note that in the pattern above, no specific function (’_’) was matched against. Instead,the fun used restricted functions to those having two arguments, the first of which is eithera list or an integer greater than 1.</p>

					<p>Be aware that extremely broad patterns with lax rate-limitting (or very high absolutelimits) may impact your node’s stability in ways <span class="code">recon_trace</span> cannot easily help you with.Similarly, tracing extremely large amounts of function calls (all of them, or all of <span class="code">io</span> for example) can be risky if more trace messages are generated than any process on the nodecould ever handle, despite the precautions taken by the library.</p>
					
					<p>In doubt, start with the most restrictive tracing possible, with low limits, and progressivelyincrease your scope.</p>

		<a id="9_4_exercises"><h3>9.4 Exercises</h3></a>
			<h4>Review Questions</h4>
				<ol>
					<il>Why is debugger use generally limited on Erlang?</il>
					<il>What are the options you can use to trace OTP processes?</il>
					<il>What determines whether a given set of functions or processes get traced?</il>
					<il>How can you stop tracing with <span class="code">recon_trace</span>? With other tools?</il>
					<il>How can you trace non-exported function calls?</il>
				</ol>
			<h4>Open-ended Questions</h4>
				<ol>
					<il>When would you want to move time stamping of traces to the VM’s trace mechanismsdirectly? What would be a possible downside of doing this?</il>
					<il>Imagine that traffic sent out of a node does so over SSL, over a multi-tenant system.However, due to wanting to validate data sent (following a customer complain), youneed to be able to inspect what was seen clear text. Can you think up a plan to be able to snoop in the data sent to their end through the <span class="code">ssl</span> socket, without snooping on the data sent to any other customer?</il>
				</ol>
			<h4>Hands-On</h4>
				<p>Using the code at <a href="https://github.com/ferd/recon_demo">https://github.com/ferd/recon_demo</a> (these may require a decent understanding of the code there):</p>
				<ol>
					<il>Can chatty processes (<span class="code">council_member</span>) message themselves? (<span class="it">hint: can this work with registered names? Do you need to check the chattiest process and see if it messages itself?</span>)</il>
					<il>Can you estimate the overall frequency at which messages are sent globally?</il>
					<il>Can you crash a node using any of the tracing tools? (<span class="it">hint: dbg makes it easier due to its greater flexibility</span>)</il>
				</ol>
<a id="conclusion"><h2>Conclusion</h2></a>
	<p>Maintaining and debugging software never ends. New bugs and confusing behaviours will keep popping up around the place all the time. There would probably be enough stuff out there to fill out dozens of manuals like this one, even when dealing with the cleanest of all systems.</p>
	<p>I hope that after reading this text, the next time stuff goes bad, it won’t go too bad. Still, there are probably going to be plenty of opportunities to debug production systems. Even the most solid bridges need to be repainted all the time in order avoid corrosion to the point of their collapse.</p>
	<p>Best of luck to you.</p>
</body>
</html>